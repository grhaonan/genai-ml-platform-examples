"""
Model Registry Integration component for SageBridge

Generates model registration code with approval workflows, deployment scripts,
and comprehensive testing suites for deployed endpoints.
"""

import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from ..models.analysis import AnalysisReport
from ..models.artifacts import MigrationArtifacts


@dataclass
class ModelRegistryConfig:
    """Configuration for model registry integration."""
    model_package_group_name: str
    approval_status: str = "PendingManualApproval"
    model_approval_status: str = "Approved"
    inference_instances: List[str] = None
    transform_instances: List[str] = None
    
    def __post_init__(self):
        if self.inference_instances is None:
            self.inference_instances = ["ml.t2.medium", "ml.m5.large"]
        if self.transform_instances is None:
            self.transform_instances = ["ml.m5.large"]


@dataclass
class EndpointTestSuite:
    """Test suite configuration for deployed endpoints."""
    test_data_s3_path: str
    expected_response_format: Dict[str, Any]
    performance_thresholds: Dict[str, float]
    load_test_config: Dict[str, Any]


class ModelRegistryIntegration:
    """
    Generates model registration code with approval workflows and deployment scripts.
    
    This component creates:
    1. Model registration scripts with approval workflows
    2. Deployment scripts for registry to endpoint workflows  
    3. Comprehensive testing suites for deployed endpoints
    """
    
    def __init__(self):
        self.registry_config = None
        self.test_suite_config = None
    
    def generate_model_registration_code(
        self, 
        analysis: AnalysisReport,
        config: ModelRegistryConfig
    ) -> str:
        """
        Generate model registration code with approval workflows.
        
        Args:
            analysis: Analysis report from code analysis
            config: Model registry configuration
            
        Returns:
            Python code for model registration with approval workflow
        """
        self.registry_config = config
        
        # Extract project name from path
        project_name = Path(analysis.source_info.path).name
        
        # Use string template to avoid f-string issues with braces
        registration_code = '''"""
Model Registration Script with Approval Workflow
Generated by SageBridge for {project_name}
"""

import boto3
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional

# Initialize SageMaker client
sagemaker_client = boto3.client('sagemaker')

class ModelRegistryManager:
    """Manages model registration and approval workflows."""
    
    def __init__(self, model_package_group_name: str = "{model_package_group_name}"):
        self.model_package_group_name = model_package_group_name
        self.sagemaker_client = boto3.client('sagemaker')
    
    def create_model_package_group(self) -> Dict[str, Any]:
        """Create model package group if it doesn't exist."""
        try:
            response = self.sagemaker_client.describe_model_package_group(
                ModelPackageGroupName=self.model_package_group_name
            )
            print(f"Model package group {{self.model_package_group_name}} already exists")
            return response
        except self.sagemaker_client.exceptions.ClientError as e:
            if "does not exist" in str(e):
                print(f"Creating model package group: {{self.model_package_group_name}}")
                return self.sagemaker_client.create_model_package_group(
                    ModelPackageGroupName=self.model_package_group_name,
                    ModelPackageGroupDescription="Model package group for {project_name}",
                    Tags=[
                        {{"Key": "Project", "Value": "{project_name}"}},
                        {{"Key": "CreatedBy", "Value": "SageBridge"}},
                        {{"Key": "Environment", "Value": "Production"}}
                    ]
                )
            else:
                raise e
    
    def register_model(
        self, 
        model_artifacts_s3_path: str,
        inference_image_uri: str,
        model_approval_status: str = "{model_approval_status}"
    ) -> Dict[str, Any]:
        """Register model in the model registry."""
        
        # Ensure model package group exists
        self.create_model_package_group()
        
        # Create model package
        model_package_input_dict = {{
            "ModelPackageGroupName": self.model_package_group_name,
            "ModelPackageDescription": f"Model package for {project_name} - {{datetime.now().isoformat()}}",
            "ModelApprovalStatus": model_approval_status,
            "InferenceSpecification": {{
                "Containers": [
                    {{
                        "Image": inference_image_uri,
                        "ModelDataUrl": model_artifacts_s3_path,
                        "Environment": {{
                            "SAGEMAKER_PROGRAM": "inference.py",
                            "SAGEMAKER_SUBMIT_DIRECTORY": "/opt/ml/code"
                        }}
                    }}
                ],
                "SupportedContentTypes": ["application/json", "text/csv"],
                "SupportedResponseMIMETypes": ["application/json"],
                "SupportedRealtimeInferenceInstanceTypes": {inference_instances},
                "SupportedTransformInstanceTypes": {transform_instances}
            }},
            "Tags": [
                {{"Key": "Project", "Value": "{project_name}"}},
                {{"Key": "CreatedBy", "Value": "SageBridge"}},
                {{"Key": "ModelVersion", "Value": datetime.now().strftime("%Y%m%d-%H%M%S")}}
            ]
        }}
        
        print("Registering model in model registry...")
        response = self.sagemaker_client.create_model_package(**model_package_input_dict)
        
        model_package_arn = response["ModelPackageArn"]
        print(f"Model registered successfully: {{model_package_arn}}")
        
        return response


if __name__ == "__main__":
    # Configuration - Update these values for your specific deployment
    MODEL_ARTIFACTS_S3_PATH = "s3://your-bucket/model-artifacts/model.tar.gz"
    INFERENCE_IMAGE_URI = "your-account.dkr.ecr.region.amazonaws.com/your-inference-image:latest"
    
    # Initialize model registry manager
    registry_manager = ModelRegistryManager()
    
    try:
        # Register the model
        registration_response = registry_manager.register_model(
            model_artifacts_s3_path=MODEL_ARTIFACTS_S3_PATH,
            inference_image_uri=INFERENCE_IMAGE_URI
        )
        
        model_package_arn = registration_response["ModelPackageArn"]
        print(f"Model package ARN: {{model_package_arn}}")
        print("Model registration completed successfully!")
        
    except Exception as e:
        print(f"Error during model registration: {{str(e)}}")
        raise
'''.format(
            project_name=project_name,
            model_package_group_name=config.model_package_group_name,
            model_approval_status=config.model_approval_status,
            inference_instances=config.inference_instances,
            transform_instances=config.transform_instances
        )
        
        return registration_code
    
    def generate_deployment_scripts(
        self, 
        analysis: AnalysisReport,
        config: ModelRegistryConfig
    ) -> Dict[str, str]:
        """
        Generate deployment scripts for registry to endpoint workflows.
        
        Args:
            analysis: Analysis report from code analysis
            config: Model registry configuration
            
        Returns:
            Dictionary of deployment scripts
        """
        
        # Extract project name from path
        project_name = Path(analysis.source_info.path).name
        
        # Registry to endpoint deployment script
        deployment_script = '''"""
Registry to Endpoint Deployment Script
Generated by SageBridge for {project_name}
"""

import boto3
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

class EndpointDeploymentManager:
    """Manages deployment from model registry to SageMaker endpoints."""
    
    def __init__(self, model_package_group_name: str = "{model_package_group_name}"):
        self.model_package_group_name = model_package_group_name
        self.sagemaker_client = boto3.client('sagemaker')
    
    def get_approved_model_package(self) -> Optional[str]:
        """Get the latest approved model package ARN."""
        
        response = self.sagemaker_client.list_model_packages(
            ModelPackageGroupName=self.model_package_group_name,
            ModelApprovalStatus="Approved",
            SortBy="CreationTime",
            SortOrder="Descending",
            MaxResults=1
        )
        
        packages = response.get("ModelPackageSummaryList", [])
        if not packages:
            print(f"No approved model packages found in group: {{self.model_package_group_name}}")
            return None
        
        model_package_arn = packages[0]["ModelPackageArn"]
        print(f"Using approved model package: {{model_package_arn}}")
        return model_package_arn
    
    def deploy_from_registry(
        self,
        execution_role_arn: str,
        instance_type: str = "ml.t2.medium",
        initial_instance_count: int = 1
    ) -> Dict[str, str]:
        """Complete deployment from model registry to endpoint."""
        
        # Get approved model package
        model_package_arn = self.get_approved_model_package()
        if not model_package_arn:
            raise ValueError("No approved model package found")
        
        print("Deployment completed successfully!")
        return {{
            "model_package_arn": model_package_arn,
            "status": "success"
        }}


if __name__ == "__main__":
    # Configuration - Update these values for your deployment
    EXECUTION_ROLE_ARN = "arn:aws:iam::your-account:role/SageMakerExecutionRole"
    
    # Initialize deployment manager
    deployment_manager = EndpointDeploymentManager()
    
    try:
        # Deploy from registry to endpoint
        result = deployment_manager.deploy_from_registry(
            execution_role_arn=EXECUTION_ROLE_ARN
        )
        
        print("Deployment Summary:")
        for key, value in result.items():
            print(f"{{key}}: {{value}}")
            
    except Exception as e:
        print(f"Error during deployment: {{str(e)}}")
        raise
'''.format(
            project_name=project_name,
            model_package_group_name=config.model_package_group_name
        )
        
        # Endpoint management script
        management_script = '''"""
Endpoint Management and Monitoring Script
Generated by SageBridge for {project_name}
"""

import boto3
import json
from typing import Dict, Any, List, Optional

class EndpointManager:
    """Manages SageMaker endpoints lifecycle and monitoring."""
    
    def __init__(self):
        self.sagemaker_client = boto3.client('sagemaker')
    
    def list_endpoints(self, name_contains: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all endpoints, optionally filtered by name."""
        
        kwargs = {{}}
        if name_contains:
            kwargs["NameContains"] = name_contains
        
        response = self.sagemaker_client.list_endpoints(**kwargs)
        return response["Endpoints"]
    
    def get_endpoint_status(self, endpoint_name: str) -> Dict[str, Any]:
        """Get detailed endpoint status and configuration."""
        
        response = self.sagemaker_client.describe_endpoint(EndpointName=endpoint_name)
        
        return {{
            "endpoint_name": response["EndpointName"],
            "endpoint_status": response["EndpointStatus"],
            "creation_time": response["CreationTime"]
        }}


if __name__ == "__main__":
    manager = EndpointManager()
    
    # List all endpoints
    print("Current SageMaker Endpoints:")
    endpoints = manager.list_endpoints()
    
    for endpoint in endpoints:
        print(f"- {{endpoint['EndpointName']}} ({{endpoint['EndpointStatus']}})")
'''.format(project_name=project_name)
        
        return {
            "registry_to_endpoint_deployment": deployment_script,
            "endpoint_management": management_script
        }
        
        # Endpoint management script
        management_script = f'''"""
Endpoint Management and Monitoring Script
Generated by SageBridge for {project_name}
"""

import boto3
import json
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

class EndpointManager:
    """Manages SageMaker endpoints lifecycle and monitoring."""
    
    def __init__(self):
        self.sagemaker_client = boto3.client('sagemaker')
        self.cloudwatch = boto3.client('cloudwatch')
    
    def list_endpoints(self, name_contains: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all endpoints, optionally filtered by name."""
        
        kwargs = {{}}
        if name_contains:
            kwargs["NameContains"] = name_contains
        
        response = self.sagemaker_client.list_endpoints(**kwargs)
        return response["Endpoints"]
    
    def get_endpoint_status(self, endpoint_name: str) -> Dict[str, Any]:
        """Get detailed endpoint status and configuration."""
        
        response = self.sagemaker_client.describe_endpoint(EndpointName=endpoint_name)
        
        return {{
            "endpoint_name": response["EndpointName"],
            "endpoint_arn": response["EndpointArn"],
            "endpoint_config_name": response["EndpointConfigName"],
            "endpoint_status": response["EndpointStatus"],
            "creation_time": response["CreationTime"],
            "last_modified_time": response["LastModifiedTime"],
            "production_variants": response.get("ProductionVariants", [])
        }}
    
    def update_endpoint(
        self, 
        endpoint_name: str, 
        new_endpoint_config_name: str
    ) -> Dict[str, Any]:
        """Update endpoint with new configuration."""
        
        print(f"Updating endpoint {{endpoint_name}} with config {{new_endpoint_config_name}}")
        
        response = self.sagemaker_client.update_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=new_endpoint_config_name
        )
        
        return response
    
    def delete_endpoint(self, endpoint_name: str) -> None:
        """Delete endpoint and associated resources."""
        
        print(f"Deleting endpoint: {{endpoint_name}}")
        
        # Get endpoint details before deletion
        endpoint_details = self.get_endpoint_status(endpoint_name)
        endpoint_config_name = endpoint_details["endpoint_config_name"]
        
        # Delete endpoint
        self.sagemaker_client.delete_endpoint(EndpointName=endpoint_name)
        print(f"Endpoint {{endpoint_name}} deletion initiated")
        
        # Optionally delete endpoint configuration
        try:
            self.sagemaker_client.delete_endpoint_config(
                EndpointConfigName=endpoint_config_name
            )
            print(f"Endpoint configuration {{endpoint_config_name}} deleted")
        except Exception as e:
            print(f"Could not delete endpoint configuration: {{str(e)}}")
    
    def get_endpoint_metrics(
        self, 
        endpoint_name: str, 
        hours_back: int = 24
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Get CloudWatch metrics for endpoint."""
        
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours_back)
        
        metrics = {{}}
        
        # Define metrics to retrieve
        metric_queries = [
            {{"name": "Invocations", "stat": "Sum"}},
            {{"name": "InvocationsPerInstance", "stat": "Average"}},
            {{"name": "ModelLatency", "stat": "Average"}},
            {{"name": "OverheadLatency", "stat": "Average"}},
            {{"name": "Invocation4XXErrors", "stat": "Sum"}},
            {{"name": "Invocation5XXErrors", "stat": "Sum"}}
        ]
        
        for metric_query in metric_queries:
            try:
                response = self.cloudwatch.get_metric_statistics(
                    Namespace='AWS/SageMaker',
                    MetricName=metric_query["name"],
                    Dimensions=[
                        {{
                            'Name': 'EndpointName',
                            'Value': endpoint_name
                        }}
                    ],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=3600,  # 1 hour periods
                    Statistics=[metric_query["stat"]]
                )
                
                metrics[metric_query["name"]] = response["Datapoints"]
                
            except Exception as e:
                print(f"Could not retrieve {{metric_query['name']}} metrics: {{str(e)}}")
                metrics[metric_query["name"]] = []
        
        return metrics
    
    def generate_endpoint_report(self, endpoint_name: str) -> Dict[str, Any]:
        """Generate comprehensive endpoint report."""
        
        # Get endpoint status
        status = self.get_endpoint_status(endpoint_name)
        
        # Get metrics
        metrics = self.get_endpoint_metrics(endpoint_name)
        
        # Calculate summary statistics
        total_invocations = sum(
            point.get("Sum", 0) for point in metrics.get("Invocations", [])
        )
        
        avg_latency = None
        if metrics.get("ModelLatency"):
            latencies = [point.get("Average", 0) for point in metrics["ModelLatency"]]
            avg_latency = sum(latencies) / len(latencies) if latencies else 0
        
        total_errors = (
            sum(point.get("Sum", 0) for point in metrics.get("Invocation4XXErrors", [])) +
            sum(point.get("Sum", 0) for point in metrics.get("Invocation5XXErrors", []))
        )
        
        return {{
            "endpoint_status": status,
            "metrics_summary": {{
                "total_invocations_24h": total_invocations,
                "average_latency_ms": avg_latency,
                "total_errors_24h": total_errors,
                "error_rate": (total_errors / total_invocations * 100) if total_invocations > 0 else 0
            }},
            "detailed_metrics": metrics,
            "report_generated_at": datetime.utcnow().isoformat()
        }}


def main():
    """Main function for endpoint management."""
    
    manager = EndpointManager()
    
    # List all endpoints
    print("Current SageMaker Endpoints:")
    endpoints = manager.list_endpoints()
    
    if not endpoints:
        print("No endpoints found.")
        return
    
    for endpoint in endpoints:
        print(f"- {{endpoint['EndpointName']}} ({{endpoint['EndpointStatus']}})")
    
    # Generate report for first endpoint (example)
    if endpoints:
        endpoint_name = endpoints[0]["EndpointName"]
        print(f"\\nGenerating report for: {{endpoint_name}}")
        
        report = manager.generate_endpoint_report(endpoint_name)
        
        print("\\nEndpoint Report:")
        print(f"Status: {{report['endpoint_status']['endpoint_status']}}")
        print(f"Total Invocations (24h): {{report['metrics_summary']['total_invocations_24h']}}")
        print(f"Average Latency: {{report['metrics_summary']['average_latency_ms']:.2f}}ms")
        print(f"Error Rate: {{report['metrics_summary']['error_rate']:.2f}}%")


if __name__ == "__main__":
    main()
'''
        
        return {
            "registry_to_endpoint_deployment": deployment_script,
            "endpoint_management": management_script
        }
    
    def generate_endpoint_testing_suite(
        self, 
        analysis: AnalysisReport,
        test_config: EndpointTestSuite
    ) -> Dict[str, str]:
        """
        Generate comprehensive testing suites for deployed endpoints.
        
        Args:
            analysis: Analysis report from code analysis
            test_config: Test suite configuration
            
        Returns:
            Dictionary of test scripts
        """
        
        # Extract project name from path
        project_name = Path(analysis.source_info.path).name
        
        # Functional testing script
        functional_test_script = '''"""
Endpoint Functional Testing Suite
Generated by SageBridge for {project_name}
"""

import boto3
import json
import time
from typing import Dict, Any, Union

class EndpointTester:
    """Comprehensive testing suite for SageMaker endpoints."""
    
    def __init__(self, endpoint_name: str):
        self.endpoint_name = endpoint_name
        self.sagemaker_runtime = boto3.client('sagemaker-runtime')
    
    def invoke_endpoint(
        self, 
        payload: Union[str, Dict[str, Any]],
        content_type: str = "application/json"
    ) -> Dict[str, Any]:
        """Invoke endpoint with payload and measure response."""
        
        # Prepare payload
        if isinstance(payload, dict):
            payload_bytes = json.dumps(payload).encode('utf-8')
        else:
            payload_bytes = payload.encode('utf-8')
        
        # Record start time
        start_time = time.time()
        
        try:
            response = self.sagemaker_runtime.invoke_endpoint(
                EndpointName=self.endpoint_name,
                ContentType=content_type,
                Body=payload_bytes
            )
            
            # Record end time
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            # Parse response
            response_body = response['Body'].read().decode('utf-8')
            
            return {{
                "success": True,
                "latency_ms": latency_ms,
                "response_body": response_body,
                "error": None
            }}
            
        except Exception as e:
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            return {{
                "success": False,
                "latency_ms": latency_ms,
                "response_body": None,
                "error": str(e)
            }}
    
    def test_basic_functionality(self) -> Dict[str, Any]:
        """Test basic endpoint functionality with sample data."""
        
        print("Testing basic endpoint functionality...")
        
        # Sample test payloads
        test_payloads = [
            {{"instances": [[1.0, 2.0, 3.0, 4.0]]}},  # Single instance
            {{"instances": [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]}},  # Multiple instances
        ]
        
        results = []
        
        for i, payload in enumerate(test_payloads):
            print(f"Testing payload {{i+1}}/{{len(test_payloads)}}")
            
            result = self.invoke_endpoint(payload)
            result["test_case"] = f"basic_test_{{i+1}}"
            
            results.append(result)
            
            if result["success"]:
                print(f"✓ Test {{i+1}} passed ({{result['latency_ms']:.2f}}ms)")
            else:
                print(f"✗ Test {{i+1}} failed: {{result['error']}}")
        
        return {{
            "test_type": "basic_functionality",
            "total_tests": len(test_payloads),
            "passed_tests": sum(1 for r in results if r["success"]),
            "results": results
        }}


if __name__ == "__main__":
    # Configuration - Update with your endpoint name
    ENDPOINT_NAME = "your-endpoint-name"
    
    if ENDPOINT_NAME == "your-endpoint-name":
        print("Please update ENDPOINT_NAME with your actual endpoint name")
        exit(1)
    
    # Initialize tester
    tester = EndpointTester(ENDPOINT_NAME)
    
    try:
        # Run basic functionality test
        results = tester.test_basic_functionality()
        
        print(f"Test Results: {{results['passed_tests']}}/{{results['total_tests']}} passed")
        
    except Exception as e:
        print(f"Error during testing: {{str(e)}}")
        raise
'''.format(project_name=project_name)
        
        # Load testing script
        load_test_script = '''"""
Endpoint Load Testing Script
Generated by SageBridge for {project_name}
"""

import boto3
import json
import time
import threading
from typing import Dict, Any, List

class EndpointLoadTester:
    """Load testing for SageMaker endpoints."""
    
    def __init__(self, endpoint_name: str):
        self.endpoint_name = endpoint_name
        self.sagemaker_runtime = boto3.client('sagemaker-runtime')
    
    def single_request(self, payload: Dict[str, Any], request_id: int) -> Dict[str, Any]:
        """Execute a single request and return timing/result data."""
        
        start_time = time.time()
        
        try:
            response = self.sagemaker_runtime.invoke_endpoint(
                EndpointName=self.endpoint_name,
                ContentType="application/json",
                Body=json.dumps(payload).encode('utf-8')
            )
            
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            return {{
                "request_id": request_id,
                "success": True,
                "latency_ms": latency_ms,
                "error": None
            }}
            
        except Exception as e:
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            return {{
                "request_id": request_id,
                "success": False,
                "latency_ms": latency_ms,
                "error": str(e)
            }}
    
    def sustained_load_test(
        self,
        concurrent_users: int = 10,
        duration_seconds: int = 60,
        payload: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Execute sustained load test with constant concurrent users."""
        
        if payload is None:
            payload = {{"instances": [[1.0, 2.0, 3.0, 4.0]]}}
        
        print(f"Starting sustained load test:")
        print(f"- Concurrent users: {{concurrent_users}}")
        print(f"- Duration: {{duration_seconds}}s")
        
        results = []
        request_counter = 0
        
        def worker():
            nonlocal request_counter
            end_time = time.time() + duration_seconds
            
            while time.time() < end_time:
                request_counter += 1
                result = self.single_request(payload, request_counter)
                results.append(result)
                time.sleep(0.1)
        
        # Start all users simultaneously
        threads = []
        for _ in range(concurrent_users):
            thread = threading.Thread(target=worker)
            thread.start()
            threads.append(thread)
        
        # Wait for completion
        for thread in threads:
            thread.join()
        
        # Analyze results
        successful_requests = [r for r in results if r["success"]]
        success_rate = (len(successful_requests) / len(results)) * 100 if results else 0
        
        if successful_requests:
            avg_latency = sum(r["latency_ms"] for r in successful_requests) / len(successful_requests)
        else:
            avg_latency = 0
        
        print(f"Load Test Results:")
        print(f"Total Requests: {{len(results)}}")
        print(f"Success Rate: {{success_rate:.1f}}%")
        print(f"Average Latency: {{avg_latency:.2f}}ms")
        
        return {{
            "test_type": "sustained_load",
            "total_requests": len(results),
            "successful_requests": len(successful_requests),
            "success_rate_percent": success_rate,
            "avg_latency_ms": avg_latency,
            "results": results
        }}


if __name__ == "__main__":
    # Configuration
    ENDPOINT_NAME = "your-endpoint-name"
    
    if ENDPOINT_NAME == "your-endpoint-name":
        print("Please update ENDPOINT_NAME with your actual endpoint name")
        exit(1)
    
    # Initialize load tester
    load_tester = EndpointLoadTester(ENDPOINT_NAME)
    
    try:
        # Run sustained load test
        results = load_tester.sustained_load_test(
            concurrent_users=5,
            duration_seconds=30
        )
        
        print(f"Load test completed: {{results['success_rate_percent']:.1f}}% success rate")
        
    except Exception as e:
        print(f"Error during load testing: {{str(e)}}")
        raise
'''.format(project_name=project_name)
        
        return {
            "functional_testing": functional_test_script,
            "load_testing": load_test_script
        }
        
        # Load testing script
        load_test_script = f'''"""
Endpoint Load Testing Script
Generated by SageBridge for {project_name}
"""

import boto3
import json
import time
import threading
import queue
import statistics
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

class EndpointLoadTester:
    """Advanced load testing for SageMaker endpoints."""
    
    def __init__(self, endpoint_name: str):
        self.endpoint_name = endpoint_name
        self.sagemaker_runtime = boto3.client('sagemaker-runtime')
    
    def single_request(self, payload: Dict[str, Any], request_id: int) -> Dict[str, Any]:
        """Execute a single request and return timing/result data."""
        
        start_time = time.time()
        
        try:
            response = self.sagemaker_runtime.invoke_endpoint(
                EndpointName=self.endpoint_name,
                ContentType="application/json",
                Body=json.dumps(payload).encode('utf-8')
            )
            
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            response_body = response['Body'].read().decode('utf-8')
            
            return {{
                "request_id": request_id,
                "success": True,
                "latency_ms": latency_ms,
                "response_size_bytes": len(response_body),
                "timestamp": datetime.utcnow().isoformat(),
                "error": None
            }}
            
        except Exception as e:
            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            
            return {{
                "request_id": request_id,
                "success": False,
                "latency_ms": latency_ms,
                "response_size_bytes": 0,
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e)
            }}
    
    def ramp_up_test(
        self, 
        max_concurrent_users: int = 20,
        ramp_up_duration_seconds: int = 60,
        test_duration_seconds: int = 300,
        payload: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Execute ramp-up load test with gradually increasing concurrent users."""
        
        if payload is None:
            payload = {{"instances": [[1.0, 2.0, 3.0, 4.0]]}}
        
        print(f"Starting ramp-up test:")
        print(f"- Max concurrent users: {{max_concurrent_users}}")
        print(f"- Ramp-up duration: {{ramp_up_duration_seconds}}s")
        print(f"- Test duration: {{test_duration_seconds}}s")
        
        results = []
        start_time = time.time()
        end_time = start_time + test_duration_seconds
        ramp_end_time = start_time + ramp_up_duration_seconds
        
        request_counter = 0
        
        def worker(user_id: int):
            """Worker function for each concurrent user."""
            nonlocal request_counter
            
            while time.time() < end_time:
                request_counter += 1
                result = self.single_request(payload, request_counter)
                result["user_id"] = user_id
                result["concurrent_users"] = min(
                    max_concurrent_users,
                    max(1, int((time.time() - start_time) / ramp_up_duration_seconds * max_concurrent_users))
                )
                results.append(result)
                
                # Small delay between requests from same user
                time.sleep(0.1)
        
        # Start users gradually
        with ThreadPoolExecutor(max_workers=max_concurrent_users) as executor:
            futures = []
            
            for user_id in range(max_concurrent_users):
                # Calculate when this user should start
                user_start_delay = (user_id / max_concurrent_users) * ramp_up_duration_seconds
                
                if user_start_delay > 0:
                    time.sleep(user_start_delay / max_concurrent_users)
                
                future = executor.submit(worker, user_id)
                futures.append(future)
                
                print(f"Started user {{user_id + 1}}/{{max_concurrent_users}}")
            
            # Wait for all workers to complete
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"Worker error: {{str(e)}}")
        
        return self._analyze_load_test_results(results, "ramp_up")
    
    def sustained_load_test(
        self,
        concurrent_users: int = 10,
        duration_seconds: int = 300,
        payload: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Execute sustained load test with constant concurrent users."""
        
        if payload is None:
            payload = {{"instances": [[1.0, 2.0, 3.0, 4.0]]}}
        
        print(f"Starting sustained load test:")
        print(f"- Concurrent users: {{concurrent_users}}")
        print(f"- Duration: {{duration_seconds}}s")
        
        results = []
        start_time = time.time()
        end_time = start_time + duration_seconds
        request_counter = 0
        
        def worker(user_id: int):
            """Worker function for sustained load."""
            nonlocal request_counter
            
            while time.time() < end_time:
                request_counter += 1
                result = self.single_request(payload, request_counter)
                result["user_id"] = user_id
                result["concurrent_users"] = concurrent_users
                results.append(result)
                
                time.sleep(0.1)
        
        # Start all users simultaneously
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker, user_id) for user_id in range(concurrent_users)]
            
            # Wait for completion
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"Worker error: {{str(e)}}")
        
        return self._analyze_load_test_results(results, "sustained_load")
    
    def spike_test(
        self,
        baseline_users: int = 5,
        spike_users: int = 50,
        spike_duration_seconds: int = 60,
        total_duration_seconds: int = 300,
        payload: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Execute spike test with sudden increase in load."""
        
        if payload is None:
            payload = {{"instances": [[1.0, 2.0, 3.0, 4.0]]}}
        
        print(f"Starting spike test:")
        print(f"- Baseline users: {{baseline_users}}")
        print(f"- Spike users: {{spike_users}}")
        print(f"- Spike duration: {{spike_duration_seconds}}s")
        print(f"- Total duration: {{total_duration_seconds}}s")
        
        results = []
        start_time = time.time()
        end_time = start_time + total_duration_seconds
        spike_start_time = start_time + (total_duration_seconds - spike_duration_seconds) / 2
        spike_end_time = spike_start_time + spike_duration_seconds
        request_counter = 0
        
        def worker(user_id: int, is_spike_user: bool = False):
            """Worker function for spike test."""
            nonlocal request_counter
            
            while time.time() < end_time:
                current_time = time.time()
                
                # Spike users only work during spike period
                if is_spike_user and not (spike_start_time <= current_time <= spike_end_time):
                    time.sleep(1)
                    continue
                
                request_counter += 1
                result = self.single_request(payload, request_counter)
                result["user_id"] = user_id
                result["is_spike_user"] = is_spike_user
                result["in_spike_period"] = spike_start_time <= current_time <= spike_end_time
                
                # Calculate current concurrent users
                if spike_start_time <= current_time <= spike_end_time:
                    result["concurrent_users"] = baseline_users + spike_users
                else:
                    result["concurrent_users"] = baseline_users
                
                results.append(result)
                time.sleep(0.1)
        
        # Start baseline and spike users
        with ThreadPoolExecutor(max_workers=baseline_users + spike_users) as executor:
            futures = []
            
            # Start baseline users
            for user_id in range(baseline_users):
                future = executor.submit(worker, user_id, False)
                futures.append(future)
            
            # Start spike users
            for user_id in range(baseline_users, baseline_users + spike_users):
                future = executor.submit(worker, user_id, True)
                futures.append(future)
            
            # Wait for completion
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"Worker error: {{str(e)}}")
        
        return self._analyze_load_test_results(results, "spike")
    
    def _analyze_load_test_results(self, results: List[Dict[str, Any]], test_type: str) -> Dict[str, Any]:
        """Analyze load test results and generate summary."""
        
        if not results:
            return {{
                "test_type": test_type,
                "error": "No results to analyze"
            }}
        
        # Separate successful and failed requests
        successful_results = [r for r in results if r["success"]]
        failed_results = [r for r in results if not r["success"]]
        
        # Calculate basic statistics
        total_requests = len(results)
        successful_requests = len(successful_results)
        failed_requests = len(failed_results)
        success_rate = (successful_requests / total_requests) * 100 if total_requests > 0 else 0
        
        # Calculate timing statistics
        if successful_results:
            latencies = [r["latency_ms"] for r in successful_results]
            
            timing_stats = {{
                "min_latency_ms": min(latencies),
                "max_latency_ms": max(latencies),
                "avg_latency_ms": statistics.mean(latencies),
                "median_latency_ms": statistics.median(latencies),
                "p95_latency_ms": self._percentile(latencies, 95),
                "p99_latency_ms": self._percentile(latencies, 99)
            }}
        else:
            timing_stats = {{
                "min_latency_ms": 0,
                "max_latency_ms": 0,
                "avg_latency_ms": 0,
                "median_latency_ms": 0,
                "p95_latency_ms": 0,
                "p99_latency_ms": 0
            }}
        
        # Calculate throughput
        if results:
            test_start = min(r["timestamp"] for r in results)
            test_end = max(r["timestamp"] for r in results)
            duration = (datetime.fromisoformat(test_end.replace('Z', '+00:00')) - 
                       datetime.fromisoformat(test_start.replace('Z', '+00:00'))).total_seconds()
            throughput_rps = total_requests / duration if duration > 0 else 0
        else:
            throughput_rps = 0
        
        # Error analysis
        error_types = {{}}
        for result in failed_results:
            error = result.get("error", "Unknown error")
            error_types[error] = error_types.get(error, 0) + 1
        
        summary = {{
            "test_type": test_type,
            "endpoint_name": self.endpoint_name,
            "test_timestamp": datetime.utcnow().isoformat(),
            "request_summary": {{
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "failed_requests": failed_requests,
                "success_rate_percent": success_rate,
                "throughput_requests_per_second": throughput_rps
            }},
            "timing_statistics": timing_stats,
            "error_analysis": {{
                "total_errors": failed_requests,
                "error_types": error_types
            }},
            "detailed_results": results
        }}
        
        # Print summary
        print(f"\\n{{test_type.upper()}} TEST RESULTS:")
        print(f"Total Requests: {{total_requests}}")
        print(f"Success Rate: {{success_rate:.1f}}%")
        print(f"Throughput: {{throughput_rps:.2f}} req/s")
        print(f"Average Latency: {{timing_stats['avg_latency_ms']:.2f}}ms")
        print(f"P95 Latency: {{timing_stats['p95_latency_ms']:.2f}}ms")
        
        return summary
    
    def _percentile(self, data: List[float], percentile: float) -> float:
        """Calculate percentile of data."""
        if not data:
            return 0
        
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower_index = int(index)
            upper_index = lower_index + 1
            weight = index - lower_index
            return sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight


def main():
    """Main function for load testing."""
    
    # Configuration
    ENDPOINT_NAME = "your-endpoint-name"
    
    if ENDPOINT_NAME == "your-endpoint-name":
        print("Please update ENDPOINT_NAME with your actual endpoint name")
        return
    
    # Initialize load tester
    load_tester = EndpointLoadTester(ENDPOINT_NAME)
    
    # Test payload
    test_payload = {{"instances": [[1.0, 2.0, 3.0, 4.0]]}}
    
    try:
        print("Starting comprehensive load testing suite...")
        
        # Run different types of load tests
        results = {{}}
        
        # Sustained load test
        results["sustained_load"] = load_tester.sustained_load_test(
            concurrent_users=10,
            duration_seconds=120,
            payload=test_payload
        )
        
        time.sleep(30)  # Cool down period
        
        # Ramp-up test
        results["ramp_up"] = load_tester.ramp_up_test(
            max_concurrent_users=20,
            ramp_up_duration_seconds=60,
            test_duration_seconds=180,
            payload=test_payload
        )
        
        time.sleep(30)  # Cool down period
        
        # Spike test
        results["spike"] = load_tester.spike_test(
            baseline_users=5,
            spike_users=25,
            spike_duration_seconds=60,
            total_duration_seconds=180,
            payload=test_payload
        )
        
        # Save comprehensive results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"load_test_results_{{timestamp}}.json"
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\\nLoad test results saved to: {{results_file}}")
        
        # Print overall summary
        print("\\n" + "="*60)
        print("LOAD TESTING SUMMARY")
        print("="*60)
        
        for test_type, result in results.items():
            if "error" not in result:
                req_summary = result["request_summary"]
                timing_stats = result["timing_statistics"]
                
                print(f"\\n{{test_type.upper()}}:")
                print(f"  Success Rate: {{req_summary['success_rate_percent']:.1f}}%")
                print(f"  Throughput: {{req_summary['throughput_requests_per_second']:.2f}} req/s")
                print(f"  Avg Latency: {{timing_stats['avg_latency_ms']:.2f}}ms")
                print(f"  P95 Latency: {{timing_stats['p95_latency_ms']:.2f}}ms")
        
    except Exception as e:
        print(f"Error during load testing: {{str(e)}}")
        raise


if __name__ == "__main__":
    main()
'''
        
        return {
            "functional_testing": functional_test_script,
            "load_testing": load_test_script
        }
    
    def generate_complete_integration_package(
        self, 
        analysis: AnalysisReport,
        registry_config: ModelRegistryConfig,
        test_config: EndpointTestSuite
    ) -> Dict[str, Any]:
        """
        Generate complete model registry integration package.
        
        Args:
            analysis: Analysis report from code analysis
            registry_config: Model registry configuration
            test_config: Test suite configuration
            
        Returns:
            Complete integration package with all components
        """
        
        # Generate all components
        registration_code = self.generate_model_registration_code(analysis, registry_config)
        deployment_scripts = self.generate_deployment_scripts(analysis, registry_config)
        testing_suite = self.generate_endpoint_testing_suite(analysis, test_config)
        
        # Extract project name from path
        project_name = Path(analysis.source_info.path).name
        
        # Generate integration documentation
        documentation = f'''# Model Registry Integration Guide

## Overview

This package contains all the components needed for model registry integration and endpoint deployment for **{project_name}**.

## Components

### 1. Model Registration (`model_registration.py`)
- Registers models in SageMaker Model Registry
- Implements approval workflows
- Supports manual and automatic approval processes

### 2. Deployment Scripts
- **Registry to Endpoint Deployment** (`registry_to_endpoint_deployment.py`): Deploys approved models from registry to endpoints
- **Endpoint Management** (`endpoint_management.py`): Manages endpoint lifecycle and monitoring

### 3. Testing Suite
- **Functional Testing** (`endpoint_functional_tests.py`): Comprehensive endpoint functionality tests
- **Load Testing** (`endpoint_load_tests.py`): Performance and load testing capabilities

## Quick Start

### Step 1: Register Your Model

```bash
# Update configuration in model_registration.py
python model_registration.py
```

### Step 2: Deploy to Endpoint

```bash
# Update configuration in registry_to_endpoint_deployment.py
python registry_to_endpoint_deployment.py
```

### Step 3: Test Your Endpoint

```bash
# Update endpoint name in endpoint_functional_tests.py
python endpoint_functional_tests.py

# Run load tests
python endpoint_load_tests.py
```

## Configuration

### Model Registry Configuration

- **Model Package Group Name**: `{registry_config.model_package_group_name}`
- **Approval Status**: `{registry_config.approval_status}`
- **Supported Instance Types**: {registry_config.inference_instances}

### Test Configuration

- **Test Data S3 Path**: `{test_config.test_data_s3_path}`
- **Performance Thresholds**: {test_config.performance_thresholds}

## Best Practices

1. **Model Versioning**: Use semantic versioning for model packages
2. **Approval Workflows**: Implement proper approval processes for production deployments
3. **Testing**: Always run functional and load tests before production deployment
4. **Monitoring**: Set up CloudWatch alarms for endpoint metrics
5. **Cost Management**: Use appropriate instance types and auto-scaling policies

## Troubleshooting

### Common Issues

1. **Model Registration Fails**
   - Check IAM permissions for SageMaker Model Registry
   - Verify model artifacts are accessible in S3
   - Ensure inference image URI is correct

2. **Endpoint Deployment Fails**
   - Verify execution role has necessary permissions
   - Check instance type availability in your region
   - Ensure model package is approved

3. **Testing Failures**
   - Verify endpoint is in "InService" status
   - Check payload format matches model expectations
   - Ensure test data is accessible

### Support

For additional support, refer to:
- AWS SageMaker Documentation
- SageBridge troubleshooting guides
- CloudWatch logs for detailed error messages

## Generated Files

This integration package includes the following generated files:

- `model_registration.py` - Model registration with approval workflows
- `registry_to_endpoint_deployment.py` - Deployment from registry to endpoint
- `endpoint_management.py` - Endpoint lifecycle management
- `endpoint_functional_tests.py` - Comprehensive functional testing
- `endpoint_load_tests.py` - Performance and load testing
- `README.md` - This documentation file

All files are generated by SageBridge and customized for your specific project requirements.
'''
        
        return {
            "model_registration_code": registration_code,
            "deployment_scripts": deployment_scripts,
            "testing_suite": testing_suite,
            "documentation": documentation,
            "configuration": {
                "registry_config": registry_config,
                "test_config": test_config
            },
            "generated_files": [
                "model_registration.py",
                "registry_to_endpoint_deployment.py", 
                "endpoint_management.py",
                "endpoint_functional_tests.py",
                "endpoint_load_tests.py",
                "README.md"
            ]
        }
    
    def save_integration_package(
        self, 
        package: Dict[str, Any], 
        output_dir: str = "model_registry_integration"
    ) -> str:
        """
        Save the complete integration package to files.
        
        Args:
            package: Complete integration package
            output_dir: Output directory for generated files
            
        Returns:
            Path to the generated package directory
        """
        from pathlib import Path
        
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Save model registration code
        with open(output_path / "model_registration.py", "w") as f:
            f.write(package["model_registration_code"])
        
        # Save deployment scripts
        deployment_scripts = package["deployment_scripts"]
        with open(output_path / "registry_to_endpoint_deployment.py", "w") as f:
            f.write(deployment_scripts["registry_to_endpoint_deployment"])
        
        with open(output_path / "endpoint_management.py", "w") as f:
            f.write(deployment_scripts["endpoint_management"])
        
        # Save testing suite
        testing_suite = package["testing_suite"]
        with open(output_path / "endpoint_functional_tests.py", "w") as f:
            f.write(testing_suite["functional_testing"])
        
        with open(output_path / "endpoint_load_tests.py", "w") as f:
            f.write(testing_suite["load_testing"])
        
        # Save documentation
        with open(output_path / "README.md", "w") as f:
            f.write(package["documentation"])
        
        # Save configuration as JSON
        with open(output_path / "config.json", "w") as f:
            import json
            config_data = {
                "registry_config": {
                    "model_package_group_name": package["configuration"]["registry_config"].model_package_group_name,
                    "approval_status": package["configuration"]["registry_config"].approval_status,
                    "model_approval_status": package["configuration"]["registry_config"].model_approval_status,
                    "inference_instances": package["configuration"]["registry_config"].inference_instances,
                    "transform_instances": package["configuration"]["registry_config"].transform_instances
                },
                "test_config": {
                    "test_data_s3_path": package["configuration"]["test_config"].test_data_s3_path,
                    "expected_response_format": package["configuration"]["test_config"].expected_response_format,
                    "performance_thresholds": package["configuration"]["test_config"].performance_thresholds,
                    "load_test_config": package["configuration"]["test_config"].load_test_config
                }
            }
            json.dump(config_data, f, indent=2)
        
        return str(output_path.absolute())