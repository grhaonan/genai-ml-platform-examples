"""
TorchScript Handler Component.

Implements dual model saving (state_dict + TorchScript) and generates 
TorchScript-compatible inference handlers with fallback loading mechanisms.
"""

import os
import json
import torch
import logging
from typing import Dict, Any, Optional, Union, List
from pathlib import Path
from dataclasses import dataclass

from ..utils.exceptions import SageMigratorError


@dataclass
class ModelSaveConfig:
    """Configuration for model saving operations."""
    save_state_dict: bool = True
    save_torchscript: bool = True
    save_onnx: bool = False
    model_name: str = "model"
    include_optimizer: bool = False
    include_metadata: bool = True


@dataclass
class InferenceConfig:
    """Configuration for inference handler generation."""
    input_shape: Optional[List[int]] = None
    input_dtype: str = "float32"
    output_shape: Optional[List[int]] = None
    preprocessing_required: bool = True
    postprocessing_required: bool = True
    batch_size: int = 1


class TorchScriptHandler:
    """
    Handles TorchScript model compatibility for SageMaker deployment.
    
    This component implements dual model saving (state_dict + TorchScript) to ensure
    compatibility with SageMaker inference containers and provides fallback mechanisms
    for model loading.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def generate_model_save_code(self, config: ModelSaveConfig) -> str:
        """
        Generate code for saving models in multiple formats.
        
        Args:
            config: Model save configuration
            
        Returns:
            Python code for model saving
        """
        save_code = f'''"""
Model saving utilities with TorchScript compatibility.
Generated by SageBridge TorchScript Handler.
"""

import os
import json
import torch
import logging
from pathlib import Path
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


def save_model_dual_format(
    model: torch.nn.Module,
    model_dir: str,
    model_name: str = "{config.model_name}",
    sample_input: Optional[torch.Tensor] = None,
    metadata: Optional[Dict[str, Any]] = None
):
    """
    Save model in both state_dict and TorchScript formats for maximum compatibility.
    
    Args:
        model: PyTorch model to save
        model_dir: Directory to save model files
        model_name: Base name for model files
        sample_input: Sample input tensor for TorchScript tracing
        metadata: Additional metadata to save with model
    """
    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"Saving model to {{model_dir}}")
    
    # Save state_dict format (standard PyTorch)
    if {str(config.save_state_dict).lower()}:
        state_dict_path = model_dir / f"{{model_name}}.pth"
        save_dict = {{
            'model_state_dict': model.state_dict(),
            'model_class': model.__class__.__name__,
            'model_module': model.__class__.__module__
        }}
        
        # Include optimizer state if requested
        if {str(config.include_optimizer).lower()} and hasattr(model, 'optimizer'):
            save_dict['optimizer_state_dict'] = model.optimizer.state_dict()
        
        torch.save(save_dict, state_dict_path)
        logger.info(f"Saved state_dict to {{state_dict_path}}")
    
    # Save TorchScript format for SageMaker compatibility
    if {str(config.save_torchscript).lower()}:
        try:
            torchscript_path = model_dir / f"{{model_name}}.pt"
            
            # Set model to evaluation mode for tracing
            model.eval()
            
            if sample_input is not None:
                # Use tracing if sample input is provided
                traced_model = torch.jit.trace(model, sample_input)
                logger.info("Using torch.jit.trace for TorchScript conversion")
            else:
                # Use scripting as fallback
                traced_model = torch.jit.script(model)
                logger.info("Using torch.jit.script for TorchScript conversion")
            
            # Save TorchScript model
            traced_model.save(str(torchscript_path))
            logger.info(f"Saved TorchScript model to {{torchscript_path}}")
            
        except Exception as e:
            logger.warning(f"Failed to save TorchScript model: {{e}}")
            logger.warning("Model will only be available in state_dict format")
    
    # Save metadata
    if {str(config.include_metadata).lower()} and metadata:
        metadata_path = model_dir / f"{{model_name}}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"Saved metadata to {{metadata_path}}")
    
    # Create model info file for inference handler
    model_info = {{
        'model_name': model_name,
        'has_state_dict': {str(config.save_state_dict).lower()},
        'has_torchscript': {str(config.save_torchscript).lower()},
        'model_class': model.__class__.__name__,
        'model_module': model.__class__.__module__
    }}
    
    info_path = model_dir / "model_info.json"
    with open(info_path, 'w') as f:
        json.dump(model_info, f, indent=2)
    
    logger.info("Model saving completed successfully")


def create_sample_input(input_shape: list, dtype: str = "float32") -> torch.Tensor:
    """
    Create sample input tensor for TorchScript tracing.
    
    Args:
        input_shape: Shape of input tensor [batch_size, ...]
        dtype: Data type for tensor
        
    Returns:
        Sample input tensor
    """
    if dtype == "float32":
        return torch.randn(input_shape, dtype=torch.float32)
    elif dtype == "int64":
        return torch.randint(0, 100, input_shape, dtype=torch.int64)
    else:
        return torch.randn(input_shape)
'''
        
        return save_code

    def generate_inference_handler(self, config: InferenceConfig) -> str:
        """
        Generate SageMaker inference handler with TorchScript compatibility.
        
        Args:
            config: Inference configuration
            
        Returns:
            Python code for inference handler
        """
        handler_code = f'''"""
SageMaker Inference Handler with TorchScript compatibility.
Generated by SageBridge TorchScript Handler.
"""

import os
import json
import torch
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Union, Optional

logger = logging.getLogger(__name__)


class ModelHandler:
    """
    SageMaker inference handler with dual model loading support.
    
    Supports both TorchScript and state_dict model formats with automatic fallback.
    """
    
    def __init__(self):
        self.model = None
        self.device = None
        self.model_info = None
        self.initialized = False
        
    def initialize(self, context):
        """
        Initialize the model handler.
        
        Args:
            context: SageMaker inference context
        """
        try:
            # Get model directory
            model_dir = context.system_properties.get("model_dir", "/opt/ml/model")
            model_dir = Path(model_dir)
            
            logger.info(f"Initializing model from {{model_dir}}")
            
            # Load model info
            info_path = model_dir / "model_info.json"
            if info_path.exists():
                with open(info_path, 'r') as f:
                    self.model_info = json.load(f)
            else:
                logger.warning("model_info.json not found, using defaults")
                self.model_info = {{"has_torchscript": True, "has_state_dict": True}}
            
            # Set device
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"Using device: {{self.device}}")
            
            # Load model with fallback mechanism
            self.model = self._load_model_with_fallback(model_dir)
            
            if self.model is None:
                raise RuntimeError("Failed to load model in any format")
            
            # Set model to evaluation mode
            self.model.eval()
            self.model.to(self.device)
            
            self.initialized = True
            logger.info("Model initialization completed successfully")
            
        except Exception as e:
            logger.error(f"Model initialization failed: {{e}}")
            raise
    
    def _load_model_with_fallback(self, model_dir: Path) -> Optional[torch.nn.Module]:
        """
        Load model with fallback mechanism: TorchScript -> state_dict.
        
        Args:
            model_dir: Directory containing model files
            
        Returns:
            Loaded PyTorch model
        """
        model = None
        
        # Try TorchScript first (preferred for SageMaker)
        if self.model_info.get("has_torchscript", False):
            torchscript_files = list(model_dir.glob("*.pt"))
            for pt_file in torchscript_files:
                try:
                    logger.info(f"Attempting to load TorchScript model from {{pt_file}}")
                    model = torch.jit.load(str(pt_file), map_location=self.device)
                    logger.info("Successfully loaded TorchScript model")
                    return model
                except Exception as e:
                    logger.warning(f"Failed to load TorchScript model from {{pt_file}}: {{e}}")
        
        # Fallback to state_dict format
        if self.model_info.get("has_state_dict", False):
            state_dict_files = list(model_dir.glob("*.pth"))
            for pth_file in state_dict_files:
                try:
                    logger.info(f"Attempting to load state_dict model from {{pth_file}}")
                    
                    # Load checkpoint
                    checkpoint = torch.load(str(pth_file), map_location=self.device)
                    
                    # This requires the model class to be available
                    # In practice, this would need to be customized based on the specific model
                    logger.warning("state_dict loading requires model class definition")
                    logger.warning("Consider using TorchScript format for better compatibility")
                    
                    # Placeholder for model instantiation
                    # model = ModelClass()  # This needs to be customized
                    # model.load_state_dict(checkpoint['model_state_dict'])
                    
                except Exception as e:
                    logger.warning(f"Failed to load state_dict model from {{pth_file}}: {{e}}")
        
        return model
    
    def preprocess(self, data: Any) -> torch.Tensor:
        """
        Preprocess input data for model inference.
        
        Args:
            data: Raw input data
            
        Returns:
            Preprocessed tensor
        """
        try:
            # Handle different input formats
            if isinstance(data, dict):
                # JSON input
                if "instances" in data:
                    inputs = data["instances"]
                elif "inputs" in data:
                    inputs = data["inputs"]
                else:
                    inputs = data
            else:
                inputs = data
            
            # Convert to tensor
            if isinstance(inputs, (list, np.ndarray)):
                tensor = torch.tensor(inputs, dtype=torch.float32)
            elif isinstance(inputs, torch.Tensor):
                tensor = inputs
            else:
                raise ValueError(f"Unsupported input type: {{type(inputs)}}")
            
            # Ensure correct shape and device
            if tensor.dim() == 1:
                tensor = tensor.unsqueeze(0)  # Add batch dimension
            
            tensor = tensor.to(self.device)
            
            # Additional preprocessing can be added here
            if {str(config.preprocessing_required).lower()}:
                tensor = self._apply_preprocessing(tensor)
            
            return tensor
            
        except Exception as e:
            logger.error(f"Preprocessing failed: {{e}}")
            raise
    
    def inference(self, data: torch.Tensor) -> torch.Tensor:
        """
        Run model inference.
        
        Args:
            data: Preprocessed input tensor
            
        Returns:
            Model output tensor
        """
        try:
            with torch.no_grad():
                output = self.model(data)
            return output
            
        except Exception as e:
            logger.error(f"Inference failed: {{e}}")
            raise
    
    def postprocess(self, data: torch.Tensor) -> Dict[str, Any]:
        """
        Postprocess model output.
        
        Args:
            data: Model output tensor
            
        Returns:
            Formatted output
        """
        try:
            # Convert to numpy for JSON serialization
            if isinstance(data, torch.Tensor):
                data = data.cpu().numpy()
            
            # Additional postprocessing can be added here
            if {str(config.postprocessing_required).lower()}:
                data = self._apply_postprocessing(data)
            
            return {{
                "predictions": data.tolist(),
                "model_info": self.model_info
            }}
            
        except Exception as e:
            logger.error(f"Postprocessing failed: {{e}}")
            raise
    
    def _apply_preprocessing(self, tensor: torch.Tensor) -> torch.Tensor:
        """Apply custom preprocessing transformations."""
        # Placeholder for custom preprocessing
        # This would be customized based on the specific model requirements
        return tensor
    
    def _apply_postprocessing(self, data: np.ndarray) -> np.ndarray:
        """Apply custom postprocessing transformations."""
        # Placeholder for custom postprocessing
        # This would be customized based on the specific model requirements
        return data


# Global model handler instance
_model_handler = ModelHandler()


def model_fn(model_dir: str) -> ModelHandler:
    """
    Load model for SageMaker inference.
    
    Args:
        model_dir: Directory containing model artifacts
        
    Returns:
        Initialized model handler
    """
    # Create a mock context for initialization
    class MockContext:
        def __init__(self, model_dir):
            self.system_properties = {{"model_dir": model_dir}}
    
    context = MockContext(model_dir)
    _model_handler.initialize(context)
    return _model_handler


def input_fn(request_body: str, content_type: str = "application/json") -> Any:
    """
    Parse input data for inference.
    
    Args:
        request_body: Raw request body
        content_type: Content type of request
        
    Returns:
        Parsed input data
    """
    if content_type == "application/json":
        return json.loads(request_body)
    elif content_type == "text/csv":
        # Handle CSV input
        import io
        import pandas as pd
        return pd.read_csv(io.StringIO(request_body)).values
    else:
        raise ValueError(f"Unsupported content type: {{content_type}}")


def predict_fn(input_data: Any, model: ModelHandler) -> Dict[str, Any]:
    """
    Run prediction on input data.
    
    Args:
        input_data: Parsed input data
        model: Model handler instance
        
    Returns:
        Prediction results
    """
    # Preprocess
    preprocessed = model.preprocess(input_data)
    
    # Inference
    output = model.inference(preprocessed)
    
    # Postprocess
    result = model.postprocess(output)
    
    return result


def output_fn(prediction: Dict[str, Any], accept: str = "application/json") -> str:
    """
    Format prediction output.
    
    Args:
        prediction: Prediction results
        accept: Accepted response format
        
    Returns:
        Formatted response
    """
    if accept == "application/json":
        return json.dumps(prediction)
    else:
        raise ValueError(f"Unsupported accept type: {{accept}}")
'''
        
        return handler_code

    def generate_compatibility_test(self) -> str:
        """
        Generate test code for TorchScript compatibility validation.
        
        Returns:
            Python test code
        """
        test_code = '''"""
TorchScript Compatibility Tests.
Generated by SageBridge TorchScript Handler.
"""

import torch
import tempfile
import json
from pathlib import Path
from typing import Dict, Any


def test_torchscript_compatibility(model: torch.nn.Module, sample_input: torch.Tensor) -> Dict[str, Any]:
    """
    Test TorchScript compatibility for a PyTorch model.
    
    Args:
        model: PyTorch model to test
        sample_input: Sample input tensor
        
    Returns:
        Compatibility test results
    """
    results = {
        "trace_compatible": False,
        "script_compatible": False,
        "output_matches": False,
        "errors": []
    }
    
    model.eval()
    
    # Get original output
    with torch.no_grad():
        original_output = model(sample_input)
    
    # Test torch.jit.trace
    try:
        traced_model = torch.jit.trace(model, sample_input)
        with torch.no_grad():
            traced_output = traced_model(sample_input)
        
        # Check if outputs match
        if torch.allclose(original_output, traced_output, rtol=1e-5, atol=1e-5):
            results["trace_compatible"] = True
            results["output_matches"] = True
        else:
            results["errors"].append("Traced model output doesn't match original")
            
    except Exception as e:
        results["errors"].append(f"Tracing failed: {str(e)}")
    
    # Test torch.jit.script
    try:
        scripted_model = torch.jit.script(model)
        with torch.no_grad():
            scripted_output = scripted_model(sample_input)
        
        if torch.allclose(original_output, scripted_output, rtol=1e-5, atol=1e-5):
            results["script_compatible"] = True
            if not results["output_matches"]:
                results["output_matches"] = True
        else:
            results["errors"].append("Scripted model output doesn't match original")
            
    except Exception as e:
        results["errors"].append(f"Scripting failed: {str(e)}")
    
    return results


def test_model_save_load_cycle(model: torch.nn.Module, sample_input: torch.Tensor) -> Dict[str, Any]:
    """
    Test complete save/load cycle for model compatibility.
    
    Args:
        model: PyTorch model to test
        sample_input: Sample input tensor
        
    Returns:
        Save/load test results
    """
    results = {
        "state_dict_cycle": False,
        "torchscript_cycle": False,
        "errors": []
    }
    
    model.eval()
    
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # Get original output
        with torch.no_grad():
            original_output = model(sample_input)
        
        # Test state_dict save/load cycle
        try:
            state_dict_path = temp_path / "model.pth"
            torch.save({
                'model_state_dict': model.state_dict(),
                'model_class': model.__class__.__name__
            }, state_dict_path)
            
            # Load and test (this would require model class reconstruction)
            checkpoint = torch.load(state_dict_path)
            # Note: In practice, this would need the model class to be available
            results["state_dict_cycle"] = True
            
        except Exception as e:
            results["errors"].append(f"State dict cycle failed: {str(e)}")
        
        # Test TorchScript save/load cycle
        try:
            torchscript_path = temp_path / "model.pt"
            
            # Try tracing first
            try:
                traced_model = torch.jit.trace(model, sample_input)
                traced_model.save(str(torchscript_path))
            except:
                # Fallback to scripting
                scripted_model = torch.jit.script(model)
                scripted_model.save(str(torchscript_path))
            
            # Load and test
            loaded_model = torch.jit.load(str(torchscript_path))
            with torch.no_grad():
                loaded_output = loaded_model(sample_input)
            
            if torch.allclose(original_output, loaded_output, rtol=1e-5, atol=1e-5):
                results["torchscript_cycle"] = True
            else:
                results["errors"].append("TorchScript loaded model output doesn't match")
                
        except Exception as e:
            results["errors"].append(f"TorchScript cycle failed: {str(e)}")
    
    return results
'''
        
        return test_code

    def create_fallback_loading_mechanism(self) -> str:
        """
        Create fallback loading mechanism for model compatibility.
        
        Returns:
            Python code for fallback loading
        """
        fallback_code = '''"""
Fallback Model Loading Mechanism.
Generated by SageBridge TorchScript Handler.
"""

import torch
import logging
from pathlib import Path
from typing import Optional, Union, Dict, Any

logger = logging.getLogger(__name__)


class ModelLoader:
    """
    Robust model loader with multiple fallback mechanisms.
    """
    
    @staticmethod
    def load_model_robust(
        model_dir: Union[str, Path],
        device: Optional[torch.device] = None
    ) -> Optional[torch.nn.Module]:
        """
        Load model with comprehensive fallback strategy.
        
        Args:
            model_dir: Directory containing model files
            device: Target device for model
            
        Returns:
            Loaded model or None if all methods fail
        """
        model_dir = Path(model_dir)
        
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"Attempting to load model from {model_dir} to {device}")
        
        # Strategy 1: TorchScript (.pt files)
        model = ModelLoader._try_torchscript_loading(model_dir, device)
        if model is not None:
            return model
        
        # Strategy 2: State dict (.pth files)
        model = ModelLoader._try_state_dict_loading(model_dir, device)
        if model is not None:
            return model
        
        # Strategy 3: ONNX (.onnx files) - if available
        model = ModelLoader._try_onnx_loading(model_dir, device)
        if model is not None:
            return model
        
        # Strategy 4: Pickle files (.pkl files)
        model = ModelLoader._try_pickle_loading(model_dir, device)
        if model is not None:
            return model
        
        logger.error("All model loading strategies failed")
        return None
    
    @staticmethod
    def _try_torchscript_loading(model_dir: Path, device: torch.device) -> Optional[torch.nn.Module]:
        """Try loading TorchScript models."""
        torchscript_files = list(model_dir.glob("*.pt"))
        
        for pt_file in torchscript_files:
            try:
                logger.info(f"Trying TorchScript loading from {pt_file}")
                model = torch.jit.load(str(pt_file), map_location=device)
                model.eval()
                logger.info("Successfully loaded TorchScript model")
                return model
            except Exception as e:
                logger.warning(f"TorchScript loading failed for {pt_file}: {e}")
        
        return None
    
    @staticmethod
    def _try_state_dict_loading(model_dir: Path, device: torch.device) -> Optional[torch.nn.Module]:
        """Try loading state dict models."""
        state_dict_files = list(model_dir.glob("*.pth"))
        
        for pth_file in state_dict_files:
            try:
                logger.info(f"Trying state dict loading from {pth_file}")
                checkpoint = torch.load(str(pth_file), map_location=device)
                
                # Check if model info is available
                info_file = model_dir / "model_info.json"
                if info_file.exists():
                    with open(info_file, 'r') as f:
                        model_info = json.load(f)
                    
                    # This would require dynamic model class loading
                    # which is complex and depends on the specific model architecture
                    logger.warning("State dict loading requires model class definition")
                    logger.warning("Consider using TorchScript for better compatibility")
                
                # For now, return None as this requires model class reconstruction
                return None
                
            except Exception as e:
                logger.warning(f"State dict loading failed for {pth_file}: {e}")
        
        return None
    
    @staticmethod
    def _try_onnx_loading(model_dir: Path, device: torch.device) -> Optional[torch.nn.Module]:
        """Try loading ONNX models (if onnx2torch is available)."""
        try:
            import onnx2torch
            onnx_files = list(model_dir.glob("*.onnx"))
            
            for onnx_file in onnx_files:
                try:
                    logger.info(f"Trying ONNX loading from {onnx_file}")
                    model = onnx2torch.convert(str(onnx_file))
                    model.to(device)
                    model.eval()
                    logger.info("Successfully loaded ONNX model")
                    return model
                except Exception as e:
                    logger.warning(f"ONNX loading failed for {onnx_file}: {e}")
        
        except ImportError:
            logger.debug("onnx2torch not available, skipping ONNX loading")
        
        return None
    
    @staticmethod
    def _try_pickle_loading(model_dir: Path, device: torch.device) -> Optional[torch.nn.Module]:
        """Try loading pickled models."""
        pickle_files = list(model_dir.glob("*.pkl"))
        
        for pkl_file in pickle_files:
            try:
                logger.info(f"Trying pickle loading from {pkl_file}")
                import pickle
                
                with open(pkl_file, 'rb') as f:
                    model = pickle.load(f)
                
                if hasattr(model, 'to'):
                    model.to(device)
                if hasattr(model, 'eval'):
                    model.eval()
                
                logger.info("Successfully loaded pickled model")
                return model
                
            except Exception as e:
                logger.warning(f"Pickle loading failed for {pkl_file}: {e}")
        
        return None
'''
        
        return fallback_code