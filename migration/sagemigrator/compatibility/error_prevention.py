"""
Error Prevention Module.

Implements proactive error prevention mechanisms for SageMaker migrations,
including model definition embedding, artifact extraction, role detection,
and retry logic.
"""

import os
import json
import time
import boto3
import tarfile
import logging
from typing import Dict, Any, Optional, List, Callable
from pathlib import Path
from dataclasses import dataclass
from functools import wraps

from ..utils.exceptions import SageMigratorError


@dataclass
class RetryConfig:
    """Configuration for retry mechanisms."""
    max_attempts: int = 3
    base_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0
    jitter: bool = True


@dataclass
class RoleConfig:
    """Configuration for execution role detection."""
    role_name: str = "SageMakerExecutionRole"
    fallback_policies: List[str] = None
    create_if_missing: bool = True
    
    def __post_init__(self):
        if self.fallback_policies is None:
            self.fallback_policies = [
                "arn:aws:iam::aws:policy/AmazonSageMakerFullAccess",
                "arn:aws:iam::aws:policy/AmazonS3FullAccess"
            ]


class ErrorPreventionModule:
    """
    Implements comprehensive error prevention mechanisms for SageMaker migrations.
    
    This module provides utilities for embedding model definitions, handling
    SageMaker artifacts, detecting execution roles, and implementing retry logic.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.iam_client = None
        self.sts_client = None
        
    def generate_embedded_evaluation_script(self, model_definition: str, 
                                          evaluation_logic: str) -> str:
        """
        Generate evaluation script with embedded model definition to prevent import errors.
        
        Args:
            model_definition: Model class definition code
            evaluation_logic: Evaluation logic code
            
        Returns:
            Complete evaluation script with embedded model
        """
        embedded_script = f'''"""
SageMaker Evaluation Script with Embedded Model Definition.
Generated by SageBridge Error Prevention Module.

This script embeds the model definition to prevent import errors
during SageMaker pipeline execution.
"""

import os
import sys
import json
import torch
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, List

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Embedded Model Definition
# This prevents import errors when the model class is not available in the container
{model_definition}

# Artifact extraction utilities
def extract_model_artifacts(model_dir: str) -> Dict[str, Any]:
    """
    Extract and prepare model artifacts from SageMaker tar.gz files.
    
    Args:
        model_dir: Directory containing model artifacts
        
    Returns:
        Dictionary with extracted artifact information
    """
    model_dir = Path(model_dir)
    artifacts_info = {{
        "extracted_files": [],
        "model_files": [],
        "config_files": [],
        "errors": []
    }}
    
    try:
        # Look for tar.gz files (common in SageMaker)
        tar_files = list(model_dir.glob("*.tar.gz"))
        
        for tar_file in tar_files:
            try:
                logger.info(f"Extracting {{tar_file}}")
                
                with tarfile.open(tar_file, 'r:gz') as tar:
                    # Extract to the same directory
                    tar.extractall(path=model_dir)
                    
                    # Record extracted files
                    extracted = tar.getnames()
                    artifacts_info["extracted_files"].extend(extracted)
                    
                    logger.info(f"Extracted {{len(extracted)}} files from {{tar_file}}")
                    
            except Exception as e:
                error_msg = f"Failed to extract {{tar_file}}: {{e}}"
                logger.error(error_msg)
                artifacts_info["errors"].append(error_msg)
        
        # Categorize extracted files
        for file_path in model_dir.rglob("*"):
            if file_path.is_file():
                if file_path.suffix in ['.pt', '.pth', '.pkl', '.onnx']:
                    artifacts_info["model_files"].append(str(file_path))
                elif file_path.suffix in ['.json', '.yaml', '.yml', '.cfg']:
                    artifacts_info["config_files"].append(str(file_path))
        
        logger.info(f"Found {{len(artifacts_info['model_files'])}} model files")
        logger.info(f"Found {{len(artifacts_info['config_files'])}} config files")
        
        return artifacts_info
        
    except Exception as e:
        error_msg = f"Artifact extraction failed: {{e}}"
        logger.error(error_msg)
        artifacts_info["errors"].append(error_msg)
        return artifacts_info


def load_model_with_fallback(model_dir: str, device: Optional[torch.device] = None) -> Optional[torch.nn.Module]:
    """
    Load model with comprehensive fallback mechanisms.
    
    Args:
        model_dir: Directory containing model files
        device: Target device for model
        
    Returns:
        Loaded model or None if loading fails
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model_dir = Path(model_dir)
    
    # First, extract any compressed artifacts
    artifacts_info = extract_model_artifacts(str(model_dir))
    
    # Try loading TorchScript models first
    for model_file in artifacts_info["model_files"]:
        if model_file.endswith('.pt'):
            try:
                logger.info(f"Attempting to load TorchScript model from {{model_file}}")
                model = torch.jit.load(model_file, map_location=device)
                model.eval()
                logger.info("Successfully loaded TorchScript model")
                return model
            except Exception as e:
                logger.warning(f"TorchScript loading failed: {{e}}")
    
    # Try loading state dict models
    for model_file in artifacts_info["model_files"]:
        if model_file.endswith('.pth'):
            try:
                logger.info(f"Attempting to load state dict from {{model_file}}")
                checkpoint = torch.load(model_file, map_location=device)
                
                # Try to instantiate the embedded model class
                # This assumes the model class is available in the current scope
                try:
                    # This would need to be customized based on the actual model class
                    # For now, we'll log a warning
                    logger.warning("State dict loading requires model class instantiation")
                    logger.warning("Consider using TorchScript format for better compatibility")
                except Exception as e:
                    logger.warning(f"Model instantiation failed: {{e}}")
                    
            except Exception as e:
                logger.warning(f"State dict loading failed: {{e}}")
    
    logger.error("All model loading attempts failed")
    return None


# Evaluation Logic with Error Handling
{evaluation_logic}


def main():
    """Main evaluation function with comprehensive error handling."""
    try:
        logger.info("Starting model evaluation")
        
        # Get model directory from environment or arguments
        model_dir = os.environ.get('SM_MODEL_DIR', '/opt/ml/model')
        
        logger.info(f"Model directory: {{model_dir}}")
        
        # Load model with fallback mechanisms
        model = load_model_with_fallback(model_dir)
        
        if model is None:
            raise RuntimeError("Failed to load model with all fallback mechanisms")
        
        # Run evaluation
        results = run_evaluation(model)
        
        # Save results
        output_dir = os.environ.get('SM_OUTPUT_DATA_DIR', '/opt/ml/output/data')
        os.makedirs(output_dir, exist_ok=True)
        
        results_path = Path(output_dir) / 'evaluation_results.json'
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Evaluation completed successfully. Results saved to {{results_path}}")
        
    except Exception as e:
        logger.error(f"Evaluation failed: {{e}}")
        raise


if __name__ == "__main__":
    main()
'''
        
        return embedded_script

    def generate_artifact_extraction_utilities(self) -> str:
        """
        Generate utilities for handling SageMaker artifact extraction.
        
        Returns:
            Python code for artifact extraction utilities
        """
        extraction_code = '''"""
SageMaker Artifact Extraction Utilities.
Generated by SageBridge Error Prevention Module.
"""

import os
import tarfile
import zipfile
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)


class ArtifactExtractor:
    """
    Handles extraction and preparation of SageMaker artifacts.
    """
    
    @staticmethod
    def extract_all_artifacts(artifact_dir: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        Extract all compressed artifacts in a directory.
        
        Args:
            artifact_dir: Directory containing artifacts
            output_dir: Output directory for extraction (defaults to artifact_dir)
            
        Returns:
            Extraction results and file inventory
        """
        artifact_dir = Path(artifact_dir)
        output_dir = Path(output_dir) if output_dir else artifact_dir
        
        results = {
            "extracted_files": [],
            "failed_extractions": [],
            "file_inventory": {
                "models": [],
                "configs": [],
                "data": [],
                "other": []
            }
        }
        
        # Extract tar.gz files
        tar_files = list(artifact_dir.glob("*.tar.gz")) + list(artifact_dir.glob("*.tgz"))
        for tar_file in tar_files:
            try:
                ArtifactExtractor._extract_tar(tar_file, output_dir, results)
            except Exception as e:
                results["failed_extractions"].append({"file": str(tar_file), "error": str(e)})
        
        # Extract zip files
        zip_files = list(artifact_dir.glob("*.zip"))
        for zip_file in zip_files:
            try:
                ArtifactExtractor._extract_zip(zip_file, output_dir, results)
            except Exception as e:
                results["failed_extractions"].append({"file": str(zip_file), "error": str(e)})
        
        # Inventory all files
        ArtifactExtractor._inventory_files(output_dir, results)
        
        return results
    
    @staticmethod
    def _extract_tar(tar_file: Path, output_dir: Path, results: Dict[str, Any]):
        """Extract tar.gz file."""
        logger.info(f"Extracting {tar_file}")
        
        with tarfile.open(tar_file, 'r:gz') as tar:
            # Security check: prevent path traversal
            for member in tar.getmembers():
                if member.name.startswith('/') or '..' in member.name:
                    logger.warning(f"Skipping potentially unsafe path: {member.name}")
                    continue
            
            tar.extractall(path=output_dir)
            extracted = tar.getnames()
            results["extracted_files"].extend(extracted)
            
        logger.info(f"Extracted {len(extracted)} files from {tar_file}")
    
    @staticmethod
    def _extract_zip(zip_file: Path, output_dir: Path, results: Dict[str, Any]):
        """Extract zip file."""
        logger.info(f"Extracting {zip_file}")
        
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            # Security check: prevent path traversal
            for member in zip_ref.namelist():
                if member.startswith('/') or '..' in member:
                    logger.warning(f"Skipping potentially unsafe path: {member}")
                    continue
            
            zip_ref.extractall(path=output_dir)
            extracted = zip_ref.namelist()
            results["extracted_files"].extend(extracted)
            
        logger.info(f"Extracted {len(extracted)} files from {zip_file}")
    
    @staticmethod
    def _inventory_files(directory: Path, results: Dict[str, Any]):
        """Create inventory of extracted files."""
        model_extensions = {'.pt', '.pth', '.pkl', '.onnx', '.h5', '.pb'}
        config_extensions = {'.json', '.yaml', '.yml', '.cfg', '.ini', '.toml'}
        data_extensions = {'.csv', '.parquet', '.json', '.txt', '.npy', '.npz'}
        
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                file_str = str(file_path)
                suffix = file_path.suffix.lower()
                
                if suffix in model_extensions:
                    results["file_inventory"]["models"].append(file_str)
                elif suffix in config_extensions:
                    results["file_inventory"]["configs"].append(file_str)
                elif suffix in data_extensions:
                    results["file_inventory"]["data"].append(file_str)
                else:
                    results["file_inventory"]["other"].append(file_str)


def prepare_sagemaker_environment():
    """
    Prepare SageMaker environment by extracting artifacts and setting up paths.
    
    This function should be called at the beginning of SageMaker scripts.
    """
    # Standard SageMaker directories
    model_dir = os.environ.get('SM_MODEL_DIR', '/opt/ml/model')
    input_dir = os.environ.get('SM_INPUT_DIR', '/opt/ml/input')
    output_dir = os.environ.get('SM_OUTPUT_DATA_DIR', '/opt/ml/output/data')
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract artifacts from model directory
    if os.path.exists(model_dir):
        logger.info(f"Extracting artifacts from {model_dir}")
        results = ArtifactExtractor.extract_all_artifacts(model_dir)
        
        # Log extraction results
        logger.info(f"Extracted {len(results['extracted_files'])} files")
        logger.info(f"Found {len(results['file_inventory']['models'])} model files")
        logger.info(f"Found {len(results['file_inventory']['configs'])} config files")
        
        if results['failed_extractions']:
            logger.warning(f"Failed to extract {len(results['failed_extractions'])} files")
            for failure in results['failed_extractions']:
                logger.warning(f"  {failure['file']}: {failure['error']}")
        
        return results
    else:
        logger.warning(f"Model directory {model_dir} does not exist")
        return None
'''
        
        return extraction_code

    def generate_role_detection_utilities(self, config: RoleConfig) -> str:
        """
        Generate utilities for execution role detection with CloudFormation fallbacks.
        
        Args:
            config: Role configuration
            
        Returns:
            Python code for role detection utilities
        """
        role_code = f'''"""
Execution Role Detection Utilities.
Generated by SageBridge Error Prevention Module.
"""

import boto3
import json
import logging
from typing import Optional, Dict, Any, List
from botocore.exceptions import ClientError, NoCredentialsError

logger = logging.getLogger(__name__)


class RoleDetector:
    """
    Detects and manages SageMaker execution roles with CloudFormation fallbacks.
    """
    
    def __init__(self):
        self.iam_client = None
        self.sts_client = None
        self.cloudformation_client = None
        
    def get_execution_role(self, role_name: str = "{config.role_name}") -> Optional[str]:
        """
        Get SageMaker execution role with comprehensive fallback mechanisms.
        
        Args:
            role_name: Name of the execution role to find
            
        Returns:
            Role ARN or None if not found
        """
        try:
            # Method 1: Try SageMaker's get_execution_role()
            try:
                from sagemaker import get_execution_role
                role_arn = get_execution_role()
                logger.info(f"Found execution role via SageMaker: {{role_arn}}")
                return role_arn
            except Exception as e:
                logger.debug(f"SageMaker get_execution_role failed: {{e}}")
            
            # Method 2: Check current assumed role
            role_arn = self._get_current_role()
            if role_arn and self._is_sagemaker_compatible_role(role_arn):
                logger.info(f"Using current assumed role: {{role_arn}}")
                return role_arn
            
            # Method 3: Look for role by name
            role_arn = self._find_role_by_name(role_name)
            if role_arn:
                logger.info(f"Found role by name: {{role_arn}}")
                return role_arn
            
            # Method 4: Check CloudFormation stacks
            role_arn = self._find_role_in_cloudformation(role_name)
            if role_arn:
                logger.info(f"Found role in CloudFormation: {{role_arn}}")
                return role_arn
            
            # Method 5: Create role if configured to do so
            if {str(config.create_if_missing).lower()}:
                role_arn = self._create_execution_role(role_name)
                if role_arn:
                    logger.info(f"Created new execution role: {{role_arn}}")
                    return role_arn
            
            logger.error("All role detection methods failed")
            return None
            
        except Exception as e:
            logger.error(f"Role detection failed: {{e}}")
            return None
    
    def _get_current_role(self) -> Optional[str]:
        """Get the currently assumed role."""
        try:
            if self.sts_client is None:
                self.sts_client = boto3.client('sts')
            
            response = self.sts_client.get_caller_identity()
            arn = response.get('Arn', '')
            
            # Extract role ARN from assumed role ARN
            if ':assumed-role/' in arn:
                # Convert assumed-role ARN to role ARN
                parts = arn.split(':assumed-role/')
                if len(parts) == 2:
                    role_session = parts[1]
                    role_name = role_session.split('/')[0]
                    account_id = response.get('Account')
                    role_arn = f"arn:aws:iam::{account_id}:role/{role_name}"
                    return role_arn
            elif ':role/' in arn:
                return arn
            
            return None
            
        except Exception as e:
            logger.debug(f"Failed to get current role: {{e}}")
            return None
    
    def _find_role_by_name(self, role_name: str) -> Optional[str]:
        """Find role by name in IAM."""
        try:
            if self.iam_client is None:
                self.iam_client = boto3.client('iam')
            
            response = self.iam_client.get_role(RoleName=role_name)
            return response['Role']['Arn']
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchEntity':
                logger.debug(f"Role {{role_name}} not found")
            else:
                logger.debug(f"Error finding role {{role_name}}: {{e}}")
            return None
        except Exception as e:
            logger.debug(f"Failed to find role by name: {{e}}")
            return None
    
    def _find_role_in_cloudformation(self, role_name: str) -> Optional[str]:
        """Find role in CloudFormation stacks."""
        try:
            if self.cloudformation_client is None:
                self.cloudformation_client = boto3.client('cloudformation')
            
            # List all stacks
            paginator = self.cloudformation_client.get_paginator('list_stacks')
            
            for page in paginator.paginate(StackStatusFilter=['CREATE_COMPLETE', 'UPDATE_COMPLETE']):
                for stack in page['StackSummaries']:
                    stack_name = stack['StackName']
                    
                    try:
                        # Get stack resources
                        resources = self.cloudformation_client.list_stack_resources(
                            StackName=stack_name
                        )
                        
                        for resource in resources['StackResourceSummaries']:
                            if (resource['ResourceType'] == 'AWS::IAM::Role' and 
                                (resource['LogicalResourceId'] == role_name or 
                                 role_name in resource.get('PhysicalResourceId', ''))):
                                
                                # Get the actual role ARN
                                role_arn = self._get_role_arn_from_resource(
                                    stack_name, resource['LogicalResourceId']
                                )
                                if role_arn:
                                    return role_arn
                                    
                    except Exception as e:
                        logger.debug(f"Error checking stack {{stack_name}}: {{e}}")
                        continue
            
            return None
            
        except Exception as e:
            logger.debug(f"Failed to find role in CloudFormation: {{e}}")
            return None
    
    def _get_role_arn_from_resource(self, stack_name: str, logical_id: str) -> Optional[str]:
        """Get role ARN from CloudFormation resource."""
        try:
            response = self.cloudformation_client.describe_stack_resource(
                StackName=stack_name,
                LogicalResourceId=logical_id
            )
            return response['StackResourceDetail']['PhysicalResourceId']
        except Exception as e:
            logger.debug(f"Failed to get role ARN from resource: {{e}}")
            return None
    
    def _is_sagemaker_compatible_role(self, role_arn: str) -> bool:
        """Check if role has SageMaker-compatible permissions."""
        try:
            if self.iam_client is None:
                self.iam_client = boto3.client('iam')
            
            role_name = role_arn.split('/')[-1]
            
            # Get attached policies
            response = self.iam_client.list_attached_role_policies(RoleName=role_name)
            
            sagemaker_policies = [
                'AmazonSageMakerFullAccess',
                'AmazonSageMakerServiceRolePolicy'
            ]
            
            for policy in response['AttachedPolicies']:
                if any(sm_policy in policy['PolicyName'] for sm_policy in sagemaker_policies):
                    return True
            
            # Check inline policies (basic check)
            inline_policies = self.iam_client.list_role_policies(RoleName=role_name)
            if inline_policies['PolicyNames']:
                # If there are inline policies, assume they might have SageMaker permissions
                return True
            
            return False
            
        except Exception as e:
            logger.debug(f"Failed to check role compatibility: {{e}}")
            # If we can't check, assume it's compatible
            return True
    
    def _create_execution_role(self, role_name: str) -> Optional[str]:
        """Create a new SageMaker execution role."""
        try:
            if self.iam_client is None:
                self.iam_client = boto3.client('iam')
            
            # Trust policy for SageMaker
            trust_policy = {{
                "Version": "2012-10-17",
                "Statement": [
                    {{
                        "Effect": "Allow",
                        "Principal": {{
                            "Service": "sagemaker.amazonaws.com"
                        }},
                        "Action": "sts:AssumeRole"
                    }}
                ]
            }}
            
            # Create role
            response = self.iam_client.create_role(
                RoleName=role_name,
                AssumeRolePolicyDocument=json.dumps(trust_policy),
                Description="SageMaker execution role created by SageBridge"
            )
            
            role_arn = response['Role']['Arn']
            
            # Attach managed policies
            for policy_arn in {config.fallback_policies}:
                try:
                    self.iam_client.attach_role_policy(
                        RoleName=role_name,
                        PolicyArn=policy_arn
                    )
                    logger.info(f"Attached policy {{policy_arn}} to role {{role_name}}")
                except Exception as e:
                    logger.warning(f"Failed to attach policy {{policy_arn}}: {{e}}")
            
            logger.info(f"Created SageMaker execution role: {{role_arn}}")
            return role_arn
            
        except Exception as e:
            logger.error(f"Failed to create execution role: {{e}}")
            return None


def get_sagemaker_role(role_name: str = "{config.role_name}") -> str:
    """
    Convenience function to get SageMaker execution role.
    
    Args:
        role_name: Name of the role to find
        
    Returns:
        Role ARN
        
    Raises:
        RuntimeError: If no suitable role is found
    """
    detector = RoleDetector()
    role_arn = detector.get_execution_role(role_name)
    
    if role_arn is None:
        raise RuntimeError(
            f"Could not find or create SageMaker execution role '{{role_name}}'. "
            "Please ensure you have appropriate IAM permissions or create the role manually."
        )
    
    return role_arn
'''
        
        return role_code

    def generate_retry_mechanisms(self, config: RetryConfig) -> str:
        """
        Generate retry logic and error handling patterns.
        
        Args:
            config: Retry configuration
            
        Returns:
            Python code for retry mechanisms
        """
        retry_code = f'''"""
Retry Mechanisms and Error Handling Patterns.
Generated by SageBridge Error Prevention Module.
"""

import time
import random
import logging
from typing import Callable, Any, Optional, Type, Tuple
from functools import wraps
from botocore.exceptions import ClientError

logger = logging.getLogger(__name__)


class RetryableError(Exception):
    """Base class for retryable errors."""
    pass


class NonRetryableError(Exception):
    """Base class for non-retryable errors."""
    pass


def retry_with_backoff(
    max_attempts: int = {config.max_attempts},
    base_delay: float = {config.base_delay},
    max_delay: float = {config.max_delay},
    exponential_base: float = {config.exponential_base},
    jitter: bool = {str(config.jitter).lower()},
    retryable_exceptions: Tuple[Type[Exception], ...] = (RetryableError, ClientError)
):
    """
    Decorator for implementing retry logic with exponential backoff.
    
    Args:
        max_attempts: Maximum number of retry attempts
        base_delay: Base delay between retries in seconds
        max_delay: Maximum delay between retries in seconds
        exponential_base: Base for exponential backoff calculation
        jitter: Whether to add random jitter to delays
        retryable_exceptions: Tuple of exception types that should trigger retries
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            last_exception = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                    
                except retryable_exceptions as e:
                    last_exception = e
                    
                    if attempt == max_attempts - 1:
                        logger.error(f"Function {{func.__name__}} failed after {{max_attempts}} attempts")
                        raise
                    
                    # Calculate delay with exponential backoff
                    delay = min(base_delay * (exponential_base ** attempt), max_delay)
                    
                    # Add jitter if enabled
                    if jitter:
                        delay *= (0.5 + random.random() * 0.5)
                    
                    logger.warning(
                        f"Function {{func.__name__}} failed on attempt {{attempt + 1}}/{{max_attempts}}: {{e}}. "
                        f"Retrying in {{delay:.2f}} seconds..."
                    )
                    
                    time.sleep(delay)
                    
                except NonRetryableError as e:
                    logger.error(f"Non-retryable error in {{func.__name__}}: {{e}}")
                    raise
                    
                except Exception as e:
                    # For unknown exceptions, don't retry by default
                    logger.error(f"Unknown error in {{func.__name__}}: {{e}}")
                    raise
            
            # This should never be reached, but just in case
            if last_exception:
                raise last_exception
                
        return wrapper
    return decorator


class SageMakerRetryHandler:
    """
    Specialized retry handler for SageMaker operations.
    """
    
    @staticmethod
    def is_retryable_error(exception: Exception) -> bool:
        """
        Determine if an exception is retryable for SageMaker operations.
        
        Args:
            exception: Exception to check
            
        Returns:
            True if the exception is retryable
        """
        if isinstance(exception, ClientError):
            error_code = exception.response.get('Error', {{}}).get('Code', '')
            
            # Retryable AWS errors
            retryable_codes = {{
                'Throttling',
                'ThrottlingException', 
                'ServiceUnavailable',
                'InternalError',
                'InternalFailure',
                'ServiceTimeout',
                'RequestTimeout',
                'TooManyRequestsException'
            }}
            
            return error_code in retryable_codes
        
        # Network-related errors are usually retryable
        if isinstance(exception, (ConnectionError, TimeoutError)):
            return True
        
        return False
    
    @staticmethod
    @retry_with_backoff(
        max_attempts=5,
        base_delay=2.0,
        max_delay=120.0,
        retryable_exceptions=(ClientError, ConnectionError, TimeoutError)
    )
    def execute_sagemaker_operation(operation: Callable, *args, **kwargs) -> Any:
        """
        Execute SageMaker operation with retry logic.
        
        Args:
            operation: SageMaker operation to execute
            *args: Positional arguments for the operation
            **kwargs: Keyword arguments for the operation
            
        Returns:
            Operation result
        """
        try:
            result = operation(*args, **kwargs)
            logger.info(f"SageMaker operation {{operation.__name__}} completed successfully")
            return result
            
        except ClientError as e:
            error_code = e.response.get('Error', {{}}).get('Code', '')
            error_message = e.response.get('Error', {{}}).get('Message', '')
            
            logger.error(f"SageMaker operation failed: {{error_code}} - {{error_message}}")
            
            if SageMakerRetryHandler.is_retryable_error(e):
                raise RetryableError(f"Retryable SageMaker error: {{error_code}} - {{error_message}}")
            else:
                raise NonRetryableError(f"Non-retryable SageMaker error: {{error_code}} - {{error_message}}")
        
        except Exception as e:
            logger.error(f"Unexpected error in SageMaker operation: {{e}}")
            raise


def create_pipeline_with_retry(pipeline_definition: Callable, *args, **kwargs):
    """
    Create SageMaker pipeline with retry logic.
    
    Args:
        pipeline_definition: Function that creates the pipeline
        *args: Arguments for pipeline creation
        **kwargs: Keyword arguments for pipeline creation
        
    Returns:
        Created pipeline
    """
    return SageMakerRetryHandler.execute_sagemaker_operation(
        pipeline_definition, *args, **kwargs
    )


def execute_pipeline_with_retry(pipeline, execution_name: Optional[str] = None):
    """
    Execute SageMaker pipeline with retry logic.
    
    Args:
        pipeline: SageMaker pipeline to execute
        execution_name: Optional name for the execution
        
    Returns:
        Pipeline execution
    """
    def _execute():
        if execution_name:
            return pipeline.start(execution_display_name=execution_name)
        else:
            return pipeline.start()
    
    return SageMakerRetryHandler.execute_sagemaker_operation(_execute)


def wait_for_pipeline_completion_with_retry(execution, max_wait_time: int = 3600):
    """
    Wait for pipeline completion with retry logic.
    
    Args:
        execution: Pipeline execution to wait for
        max_wait_time: Maximum time to wait in seconds
        
    Returns:
        Final execution status
    """
    @retry_with_backoff(max_attempts=3, base_delay=5.0)
    def _wait():
        return execution.wait(max_wait_time)
    
    return _wait()


# Example usage functions
def robust_model_deployment(model, endpoint_name: str, instance_type: str = 'ml.t2.medium'):
    """
    Deploy model to endpoint with comprehensive error handling.
    
    Args:
        model: SageMaker model to deploy
        endpoint_name: Name for the endpoint
        instance_type: Instance type for deployment
        
    Returns:
        Deployed endpoint
    """
    @retry_with_backoff(max_attempts=3, base_delay=10.0, max_delay=300.0)
    def _deploy():
        return model.deploy(
            initial_instance_count=1,
            instance_type=instance_type,
            endpoint_name=endpoint_name
        )
    
    return _deploy()
'''
        
        return retry_code

    def _initialize_aws_clients(self):
        """Initialize AWS clients if not already initialized."""
        try:
            if self.iam_client is None:
                self.iam_client = boto3.client('iam')
            if self.sts_client is None:
                self.sts_client = boto3.client('sts')
        except Exception as e:
            self.logger.warning(f"Failed to initialize AWS clients: {e}")

    def generate_diagnostic_utilities(self) -> str:
        """
        Generate diagnostic utilities for troubleshooting common issues.
        
        Returns:
            Python code for diagnostic utilities
        """
        diagnostic_code = '''"""
Diagnostic Utilities for SageMaker Troubleshooting.
Generated by SageBridge Error Prevention Module.
"""

import os
import json
import boto3
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class SageMakerDiagnostics:
    """
    Diagnostic utilities for troubleshooting SageMaker issues.
    """
    
    def __init__(self):
        self.sagemaker_client = boto3.client('sagemaker')
        self.logs_client = boto3.client('logs')
        self.s3_client = boto3.client('s3')
    
    def diagnose_training_job(self, job_name: str) -> Dict[str, Any]:
        """
        Diagnose issues with a SageMaker training job.
        
        Args:
            job_name: Name of the training job
            
        Returns:
            Diagnostic information
        """
        diagnosis = {
            "job_status": None,
            "failure_reason": None,
            "logs": [],
            "recommendations": []
        }
        
        try:
            # Get job details
            response = self.sagemaker_client.describe_training_job(TrainingJobName=job_name)
            
            diagnosis["job_status"] = response.get("TrainingJobStatus")
            diagnosis["failure_reason"] = response.get("FailureReason")
            
            # Get logs
            log_group = f"/aws/sagemaker/TrainingJobs/{job_name}"
            diagnosis["logs"] = self._get_cloudwatch_logs(log_group)
            
            # Generate recommendations based on common issues
            diagnosis["recommendations"] = self._generate_training_recommendations(response)
            
        except Exception as e:
            diagnosis["error"] = str(e)
            logger.error(f"Failed to diagnose training job {job_name}: {e}")
        
        return diagnosis
    
    def diagnose_pipeline_execution(self, execution_arn: str) -> Dict[str, Any]:
        """
        Diagnose issues with a SageMaker pipeline execution.
        
        Args:
            execution_arn: ARN of the pipeline execution
            
        Returns:
            Diagnostic information
        """
        diagnosis = {
            "execution_status": None,
            "failed_steps": [],
            "step_details": {},
            "recommendations": []
        }
        
        try:
            # Get execution details
            response = self.sagemaker_client.describe_pipeline_execution(
                PipelineExecutionArn=execution_arn
            )
            
            diagnosis["execution_status"] = response.get("PipelineExecutionStatus")
            
            # Get step details
            steps_response = self.sagemaker_client.list_pipeline_execution_steps(
                PipelineExecutionArn=execution_arn
            )
            
            for step in steps_response.get("PipelineExecutionSteps", []):
                step_name = step["StepName"]
                step_status = step["StepStatus"]
                
                diagnosis["step_details"][step_name] = {
                    "status": step_status,
                    "failure_reason": step.get("FailureReason"),
                    "metadata": step.get("Metadata", {})
                }
                
                if step_status == "Failed":
                    diagnosis["failed_steps"].append(step_name)
            
            # Generate recommendations
            diagnosis["recommendations"] = self._generate_pipeline_recommendations(diagnosis)
            
        except Exception as e:
            diagnosis["error"] = str(e)
            logger.error(f"Failed to diagnose pipeline execution {execution_arn}: {e}")
        
        return diagnosis
    
    def _get_cloudwatch_logs(self, log_group: str, max_lines: int = 100) -> List[str]:
        """Get recent CloudWatch logs."""
        try:
            response = self.logs_client.describe_log_streams(
                logGroupName=log_group,
                orderBy='LastEventTime',
                descending=True,
                limit=1
            )
            
            if not response.get('logStreams'):
                return ["No log streams found"]
            
            log_stream = response['logStreams'][0]['logStreamName']
            
            events_response = self.logs_client.get_log_events(
                logGroupName=log_group,
                logStreamName=log_stream,
                limit=max_lines
            )
            
            return [event['message'] for event in events_response.get('events', [])]
            
        except Exception as e:
            logger.error(f"Failed to get logs from {log_group}: {e}")
            return [f"Failed to retrieve logs: {e}"]
    
    def _generate_training_recommendations(self, job_details: Dict[str, Any]) -> List[str]:
        """Generate recommendations for training job issues."""
        recommendations = []
        
        failure_reason = job_details.get("FailureReason", "").lower()
        
        if "out of memory" in failure_reason or "oom" in failure_reason:
            recommendations.extend([
                "Consider using a larger instance type with more memory",
                "Reduce batch size in your training script",
                "Implement gradient checkpointing to reduce memory usage"
            ])
        
        if "no module named" in failure_reason:
            recommendations.extend([
                "Check that all required packages are in requirements.txt",
                "Verify package names and versions are correct",
                "Consider using a custom container with pre-installed dependencies"
            ])
        
        if "permission denied" in failure_reason or "access denied" in failure_reason:
            recommendations.extend([
                "Check that the execution role has necessary permissions",
                "Verify S3 bucket policies allow access from SageMaker",
                "Ensure the role can access required AWS services"
            ])
        
        if "timeout" in failure_reason:
            recommendations.extend([
                "Increase the max_run parameter in your estimator",
                "Optimize your training code for better performance",
                "Consider using spot instances with checkpointing"
            ])
        
        return recommendations
    
    def _generate_pipeline_recommendations(self, diagnosis: Dict[str, Any]) -> List[str]:
        """Generate recommendations for pipeline issues."""
        recommendations = []
        
        if diagnosis["failed_steps"]:
            recommendations.append(f"Focus on failed steps: {', '.join(diagnosis['failed_steps'])}")
        
        for step_name, details in diagnosis["step_details"].items():
            if details["status"] == "Failed":
                failure_reason = details.get("failure_reason", "").lower()
                
                if "resource" in failure_reason and "limit" in failure_reason:
                    recommendations.append(f"Step {step_name}: Consider requesting service limit increase")
                
                if "role" in failure_reason or "permission" in failure_reason:
                    recommendations.append(f"Step {step_name}: Check execution role permissions")
        
        return recommendations


def run_comprehensive_diagnostics(
    training_job_name: Optional[str] = None,
    pipeline_execution_arn: Optional[str] = None
) -> Dict[str, Any]:
    """
    Run comprehensive diagnostics for SageMaker resources.
    
    Args:
        training_job_name: Optional training job to diagnose
        pipeline_execution_arn: Optional pipeline execution to diagnose
        
    Returns:
        Complete diagnostic report
    """
    diagnostics = SageMakerDiagnostics()
    report = {
        "timestamp": time.time(),
        "environment": {
            "aws_region": os.environ.get("AWS_DEFAULT_REGION", "unknown"),
            "sagemaker_role": os.environ.get("SAGEMAKER_ROLE", "unknown")
        }
    }
    
    if training_job_name:
        report["training_job"] = diagnostics.diagnose_training_job(training_job_name)
    
    if pipeline_execution_arn:
        report["pipeline_execution"] = diagnostics.diagnose_pipeline_execution(pipeline_execution_arn)
    
    return report
'''
        
        return diagnostic_code