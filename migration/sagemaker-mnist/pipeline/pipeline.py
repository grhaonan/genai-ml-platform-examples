#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SageMaker Pipeline with Train, Evaluate, and Conditional Register Steps
Generated by SageMigrator CLI on 2026-01-13 09:14:50
Processor Type: sklearn
"""

# Dependency validation
import sys
import subprocess

def check_dependencies():
    """Check if required dependencies are installed"""
    # Import the dependency validator
    try:
        from pathlib import Path
        import importlib.util
        
        # Try to import the validator from the same package
        validator_path = Path(__file__).parent.parent / "sagemigrator" / "utils" / "dependency_validator.py"
        if validator_path.exists():
            spec = importlib.util.spec_from_file_location("dependency_validator", validator_path)
            validator_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(validator_module)
            
            # Use the comprehensive validator
            if not validator_module.validate_pipeline_environment():
                sys.exit(1)
        else:
            # Fallback to simple validation
            required_packages = [
                ('sagemaker', '2.190.0'),
                ('boto3', '1.26.0'),
                ('pandas', '1.5.0'),
                ('numpy', '1.21.0')
            ]
            
            missing_packages = []
            
            for package, min_version in required_packages:
                try:
                    __import__(package)
                except ImportError:
                    missing_packages.append(f"{package}>={min_version}")
            
            if missing_packages:
                print("âŒ Missing required dependencies:")
                for pkg in missing_packages:
                    print(f"   - {pkg}")
                print("\nðŸ“¦ Install missing packages:")
                print(f"   pip install {' '.join(missing_packages)}")
                print("\nðŸ’¡ Or install from requirements.txt:")
                print("   pip install -r requirements.txt")
                print("\nðŸ”§ For SageMaker SDK v2.x compatibility:")
                print("   pip install 'sagemaker>=2.190.0,<3.0.0' --force-reinstall")
                sys.exit(1)
    except Exception as e:
        print(f"âš ï¸  Warning: Could not validate dependencies: {e}")
        print("Proceeding with pipeline execution...")

def check_role_permissions():
    """Check if the SageMaker execution role is valid"""
    try:
        from pathlib import Path
        import importlib.util
        
        # Try to import the role validator
        validator_path = Path(__file__).parent.parent / "sagemigrator" / "utils" / "role_validator.py"
        if validator_path.exists():
            spec = importlib.util.spec_from_file_location("role_validator", validator_path)
            validator_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(validator_module)
            
            # Validate the role that will be used by the pipeline
            # This is done during pipeline initialization in _setup_execution_role
            print("âœ… Role validation will be performed during pipeline initialization")
        else:
            print("âš ï¸  Role validator not available - role will be validated during execution")
    except Exception as e:
        print(f"âš ï¸  Warning: Could not validate role: {e}")

def check_s3_bucket():
    """Check if the S3 bucket exists and is accessible"""
    try:
        from pathlib import Path
        import importlib.util
        
        # Try to import the S3 bucket validator
        validator_path = Path(__file__).parent.parent / "sagemigrator" / "utils" / "s3_bucket_validator.py"
        if validator_path.exists():
            spec = importlib.util.spec_from_file_location("s3_bucket_validator", validator_path)
            validator_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(validator_module)
            
            # Validate the bucket that will be used by the pipeline
            # This is done during pipeline initialization in _setup_s3_bucket
            print("âœ… S3 bucket validation will be performed during pipeline initialization")
        else:
            print("âš ï¸  S3 bucket validator not available - bucket will be validated during execution")
    except Exception as e:
        print(f"âš ï¸  Warning: Could not validate S3 bucket: {e}")

# Check dependencies, role, and S3 bucket before importing SageMaker modules
check_dependencies()
check_role_permissions()
check_s3_bucket()

import boto3
import sagemaker
from sagemaker.pytorch import PyTorch
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import TrainingStep, ProcessingStep
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo
from sagemaker.workflow.condition_step import ConditionStep
from sagemaker.workflow.functions import JsonGet
from sagemaker.workflow.parameters import (
    ParameterString, 
    ParameterInteger, 
    ParameterFloat
)
from sagemaker.workflow.properties import PropertyFile
from sagemaker.inputs import TrainingInput
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ScriptProcessor

from sagemaker.model_metrics import MetricsSource, ModelMetrics
import json
import os
from datetime import datetime
from pathlib import Path


class SagemigratorPipeline:
    """
    SageMaker Pipeline for automated ML workflow
    """
    
    def __init__(self, 
                 role: str = None,
                 bucket: str = None,
                 region: str = "us-east-1",
                 processor_type: str = "sklearn"):  # Added processor_type parameter
        """
        Initialize the pipeline
        
        Args:
            role: SageMaker execution role ARN
            bucket: S3 bucket for artifacts
            region: AWS region
            processor_type: Processor type for evaluation step
        """
        # Get script directory for relative file paths
        self.script_dir = Path(__file__).parent.absolute()
        
        # Set region first as it's needed by validation methods
        self.region = region
        self.processor_type = processor_type  # Store processor type
        self.framework_version = "2.1.0"  # Store framework version
        
        # Validate and set up role (now that self.region is available)
        self._setup_execution_role(role)
        
        # Validate and set up S3 bucket (now that self.region is available)
        self._setup_s3_bucket(bucket)
        
        # Initialize SageMaker session
        self.sagemaker_session = sagemaker.Session(boto3.Session(region_name=self.region))
        
        # Pipeline parameters
        self.setup_parameters()
        
    def setup_parameters(self):
        """Setup pipeline parameters"""
        self.input_data = ParameterString(
            name="InputData",
            default_value=f"s3://{self.bucket}/data"
        )
        
        self.model_approval_status = ParameterString(
            name="ModelApprovalStatus",
            default_value="PendingManualApproval"
        )
        
        self.accuracy_threshold = ParameterFloat(
            name="AccuracyThreshold",
            default_value=0.85
        )
        
        self.instance_type = ParameterString(
            name="TrainingInstanceType",
            default_value="ml.c5.xlarge"
        )
        
        self.instance_count = ParameterInteger(
            name="TrainingInstanceCount",
            default_value=1
        )
        
        self.epochs = ParameterInteger(
            name="Epochs",
            default_value=10
        )
        
        self.batch_size = ParameterInteger(
            name="BatchSize",
            default_value=64
        )
        
        self.learning_rate = ParameterFloat(
            name="LearningRate",
            default_value=0.001
        )
    
    def _setup_execution_role(self, role: str = None):
        """Setup SageMaker execution role from CloudFormation stack outputs"""
        try:
            # If role is explicitly provided, use it
            if role is not None:
                self.role = role
                print(f"âœ… Using provided SageMaker role: {self.role}")
                return
            
            # Try to import the CloudFormation utilities
            from pathlib import Path
            import importlib.util
            
            # Adjust path based on where the generated pipeline is located
            cf_utils_path = Path(__file__).parent.parent / "sagemigrator" / "utils" / "cloudformation_utils.py"
            if not cf_utils_path.exists():
                # Try alternative path for generated pipelines in different locations
                cf_utils_path = Path(__file__).parent.parent.parent / "sagemigrator" / "utils" / "cloudformation_utils.py"
            if cf_utils_path.exists():
                spec = importlib.util.spec_from_file_location("cloudformation_utils", cf_utils_path)
                cf_utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(cf_utils_module)
                
                # Get deployment resources from CloudFormation stack
                resources = cf_utils_module.get_deployment_resources(
                    project_name="sagemigrator",
                    region=self.region
                )
                
                if resources['role_arn']:
                    self.role = resources['role_arn']
                    print(f"âœ… Using execution role from CloudFormation stack '{resources['stack_name']}': {self.role}")
                else:
                    print("âŒ Failed to get execution role from CloudFormation stack")
                    if resources['error']:
                        print(f"   Error: {resources['error']}")
                    
                    # Fallback to default role pattern
                    import boto3
                    sts = boto3.client('sts')
                    account_id = sts.get_caller_identity()['Account']
                    self.role = f"arn:aws:iam::{account_id}:role/sagemigrator-SageMaker-ExecutionRole-dev"
                    print(f"âš ï¸  Using fallback role pattern: {self.role}")
                    print("ðŸ’¡ Hint: Run 'sagemigrator deploy' first to create the CloudFormation stack")
            else:
                # Fallback role setup if CloudFormation utils not available
                import boto3
                sts = boto3.client('sts')
                account_id = sts.get_caller_identity()['Account']
                self.role = f"arn:aws:iam::{account_id}:role/sagemigrator-SageMaker-ExecutionRole-dev"
                print(f"âš ï¸  CloudFormation utilities not available, using default role: {self.role}")
        
        except Exception as e:
            print(f"âš ï¸  Failed to get role from CloudFormation stack: {e}")
            # Fallback to default role pattern
            import boto3
            sts = boto3.client('sts')
            account_id = sts.get_caller_identity()['Account']
            self.role = f"arn:aws:iam::{account_id}:role/sagemigrator-SageMaker-ExecutionRole-dev"
            print(f"âš ï¸  Using fallback role: {self.role}")
    
    def _setup_s3_bucket(self, bucket: str = None):
        """Setup S3 bucket from CloudFormation stack outputs"""
        try:
            # If bucket is explicitly provided, use it
            if bucket is not None:
                self.bucket = bucket
                print(f"âœ… Using provided S3 bucket: {self.bucket}")
                return
            
            # Try to import the CloudFormation utilities
            from pathlib import Path
            import importlib.util
            
            # Adjust path based on where the generated pipeline is located
            cf_utils_path = Path(__file__).parent.parent / "sagemigrator" / "utils" / "cloudformation_utils.py"
            if not cf_utils_path.exists():
                # Try alternative path for generated pipelines in different locations
                cf_utils_path = Path(__file__).parent.parent.parent / "sagemigrator" / "utils" / "cloudformation_utils.py"
            if cf_utils_path.exists():
                spec = importlib.util.spec_from_file_location("cloudformation_utils", cf_utils_path)
                cf_utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(cf_utils_module)
                
                # Get deployment resources from CloudFormation stack
                resources = cf_utils_module.get_deployment_resources(
                    project_name="sagemigrator",
                    region=self.region
                )
                
                if resources['bucket_name']:
                    self.bucket = resources['bucket_name']
                    print(f"âœ… Using S3 bucket from CloudFormation stack '{resources['stack_name']}': {self.bucket}")
                else:
                    print("âŒ Failed to get S3 bucket from CloudFormation stack")
                    if resources['error']:
                        print(f"   Error: {resources['error']}")
                    
                    # Fallback to default bucket pattern
                    import boto3
                    sts = boto3.client('sts')
                    account_id = sts.get_caller_identity()['Account']
                    self.bucket = f"sagemigrator-sagemaker-bucket-{account_id}-{self.region}"
                    print(f"âš ï¸  Using fallback bucket pattern: {self.bucket}")
                    print("ðŸ’¡ Hint: Run 'sagemigrator deploy' first to create the CloudFormation stack")
            else:
                # Fallback bucket setup if CloudFormation utils not available
                import boto3
                sts = boto3.client('sts')
                account_id = sts.get_caller_identity()['Account']
                self.bucket = f"sagemigrator-sagemaker-bucket-{account_id}-{self.region}"
                print(f"âš ï¸  CloudFormation utilities not available, using default bucket: {self.bucket}")
        
        except Exception as e:
            print(f"âš ï¸  Failed to get bucket from CloudFormation stack: {e}")
            # Fallback to default bucket pattern
            import boto3
            sts = boto3.client('sts')
            account_id = sts.get_caller_identity()['Account']
            self.bucket = f"sagemigrator-sagemaker-bucket-{account_id}-{self.region}"
            print(f"âš ï¸  Using fallback bucket: {self.bucket}")
    
    def create_preprocessing_step(self):
        """Create data preprocessing step"""
        
        # Create processor for data preprocessing
        processor = SKLearnProcessor(
            framework_version="1.2-1",
            role=self.role,
            instance_type="ml.c5.xlarge",
            instance_count=1,
            sagemaker_session=self.sagemaker_session
        )
        
        # Create preprocessing step - directly use Python script instead of shell wrapper
        step_preprocess = ProcessingStep(
            name="PreprocessData",
            processor=processor,
            code=str(self.script_dir / "preprocessing.py"),
            inputs=[],
            outputs=[
                ProcessingOutput(
                    output_name="train",
                    source="/opt/ml/processing/train",
                    destination=f"s3://{self.bucket}/data/train"
                ),
                ProcessingOutput(
                    output_name="test",
                    source="/opt/ml/processing/test", 
                    destination=f"s3://{self.bucket}/data/test"
                )
            ]
        )
        
        return step_preprocess
    
    def create_training_step(self, preprocessing_step):
        """Create training step"""
        
        # Create PyTorch estimator
        estimator = PyTorch(
            entry_point="train.py",
            source_dir=str(self.script_dir),  # Point to the current training directory
            role=self.role,
            instance_type=self.instance_type,
            instance_count=self.instance_count,
            framework_version="2.1.0",
            py_version="py310",
            hyperparameters={
                'epochs': self.epochs,
                'batch-size': self.batch_size,
                'lr': self.learning_rate
            },
            use_spot_instances=True,
            max_run=3600,
            max_wait=7200,
            checkpoint_s3_uri=f"s3://{self.bucket}/checkpoints",
            sagemaker_session=self.sagemaker_session,
            tags=[
                {'Key': 'Project', 'Value': 'sagemigrator'},
                {'Key': 'Pipeline', 'Value': 'automated-training'},
                {'Key': 'GeneratedBy', 'Value': 'sagemigrator-cli'}
            ]
        )
        
        # Create training step with preprocessed data as input
        step_train = TrainingStep(
            name="TrainModel",
            estimator=estimator,
            inputs={
                "training": TrainingInput(
                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs["train"].S3Output.S3Uri,
                    content_type="application/x-parquet"
                ),
                "testing": TrainingInput(
                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs["test"].S3Output.S3Uri,
                    content_type="application/x-parquet"
                )
            }
        )
        
        return step_train
    
    def create_evaluation_step(self, training_step):
        """Create model evaluation step"""
        
        # Use SKLearnProcessor for evaluation to avoid Docker image issues
        # SKLearn processor is more reliable and has all necessary dependencies
        processor = SKLearnProcessor(
            framework_version="1.2-1",
            role=self.role,
            instance_type="ml.c5.xlarge",
            instance_count=1,
            sagemaker_session=self.sagemaker_session
        )
        
        # Create evaluation step with SKLearn processor (more reliable)
        step_eval = ProcessingStep(
            name="EvaluateModel",
            processor=processor,
            code=str(self.script_dir / "evaluation.py"),
            inputs=[
                ProcessingInput(
                    source=training_step.properties.ModelArtifacts.S3ModelArtifacts,
                    destination="/opt/ml/processing/model"
                )
            ],
            outputs=[
                ProcessingOutput(
                    output_name="evaluation",
                    source="/opt/ml/processing/evaluation",
                    destination=f"s3://{self.bucket}/evaluation"
                )
            ],
            property_files=[
                PropertyFile(
                    name="EvaluationReport",
                    output_name="evaluation",
                    path="evaluation.json"
                )
            ]
        )
        
        return step_eval
    
    def create_model_registration_step(self, training_step, evaluation_step):
        """Create conditional model registration step"""
        
        # Create model metrics
        model_metrics = ModelMetrics(
            model_statistics=MetricsSource(
                s3_uri=f"{evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']}/model_metrics.json",
                content_type="application/json"
            )
        )
        
        # Create model registration step
        step_register = RegisterModel(
            name="RegisterModel",
            estimator=training_step.estimator,
            model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
            content_types=["application/json"],
            response_types=["application/json"],
            inference_instances=["ml.t2.medium", "ml.m5.large"],
            transform_instances=["ml.m5.large"],
            model_package_group_name=f"sagemigrator-model-group",
            approval_status=self.model_approval_status,
            model_metrics=model_metrics,
            description=f"sagemigrator model trained with SageMigrator pipeline"
        )
        
        return step_register
    
    def create_conditional_registration_step(self, evaluation_step, training_step):
        """Create conditional registration based on model performance"""
        
        # Condition for model approval
        cond_gte = ConditionGreaterThanOrEqualTo(
            left=JsonGet(
                step_name=evaluation_step.name,
                property_file=evaluation_step.property_files[0],
                json_path="classification_metrics.accuracy"
            ),
            right=self.accuracy_threshold
        )
        
        # Create comprehensive model metrics including both training and evaluation metrics
        model_metrics = ModelMetrics(
            model_statistics=MetricsSource(
                s3_uri=f"{evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']}/comprehensive_model_metrics.json",
                content_type="application/json"
            ),
            model_data_statistics=MetricsSource(
                s3_uri=f"{evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']}/training_metrics.json",
                content_type="application/json"
            )
        )
        
        # Auto-approve model if accuracy threshold is met
        step_approve = RegisterModel(
            name="AutoApproveModel",
            estimator=training_step.estimator,
            model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
            content_types=["application/json"],
            response_types=["application/json"],
            inference_instances=["ml.t2.medium", "ml.c5.xlarge"],
            transform_instances=["ml.c5.xlarge"],
            model_package_group_name=f"sagemigrator-model-group",
            approval_status="Approved",  # Fixed: Use string instead of parameter
            model_metrics=model_metrics,
            description=f"Auto-approved sagemigrator model meeting accuracy threshold (â‰¥0.85)"
        )
        
        # Manual approval model if accuracy threshold is not met
        step_manual = RegisterModel(
            name="ManualApprovalModel",
            estimator=training_step.estimator,
            model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
            content_types=["application/json"],
            response_types=["application/json"],
            inference_instances=["ml.t2.medium", "ml.c5.xlarge"],
            transform_instances=["ml.c5.xlarge"],
            model_package_group_name=f"sagemigrator-model-group",
            approval_status=self.model_approval_status,
            model_metrics=model_metrics,
            description=f"sagemigrator model requiring manual approval (accuracy <0.85)"
        )
        
        # Conditional step
        step_cond = ConditionStep(
            name="CheckAccuracyCondition",
            conditions=[cond_gte],
            if_steps=[step_approve],
            else_steps=[step_manual]
        )
        
        return step_cond
    
    def create_pipeline(self):
        """Create the complete pipeline"""
        
        # Create pipeline steps
        step_preprocess = self.create_preprocessing_step()
        step_train = self.create_training_step(step_preprocess)
        step_eval = self.create_evaluation_step(step_train)
        step_cond = self.create_conditional_registration_step(step_eval, step_train)
        
        # Create pipeline
        pipeline = Pipeline(
            name=f"sagemigrator-pipeline",
            parameters=[
                self.input_data,
                self.model_approval_status,
                self.accuracy_threshold,
                self.instance_type,
                self.instance_count,
                self.epochs,
                self.batch_size,
                self.learning_rate
            ],
            steps=[
                step_preprocess,
                step_train,
                step_eval,
                step_cond
            ],
            sagemaker_session=self.sagemaker_session
        )
        
        return pipeline
    
    def deploy_pipeline(self):
        """Deploy the pipeline to SageMaker"""
        pipeline = self.create_pipeline()
        
        try:
            # Create or update pipeline
            pipeline.upsert(role_arn=self.role)
            print(f"âœ… Pipeline '{pipeline.name}' deployed successfully!")
            
            return pipeline
            
        except Exception as e:
            print(f"âŒ Pipeline deployment failed: {e}")
            raise
    
    def execute_pipeline(self, parameters=None):
        """Execute the pipeline with optional parameters"""
        pipeline = self.create_pipeline()
        
        execution_parameters = parameters or {}
        
        try:
            execution = pipeline.start(parameters=execution_parameters)
            print(f"âœ… Pipeline execution started: {execution.arn}")
            
            return execution
            
        except Exception as e:
            print(f"âŒ Pipeline execution failed: {e}")
            raise


def main():
    """Main function to deploy and execute the pipeline"""
    
    # Get AWS account ID
    import boto3
    sts = boto3.client('sts')
    account_id = sts.get_caller_identity()['Account']
    
    # Create pipeline with deployment-specific configuration
    # The pipeline will automatically fetch role and bucket from CloudFormation stack
    pipeline_manager = SagemigratorPipeline(
        region="us-east-1"
    )
    
    # Deploy pipeline
    pipeline = pipeline_manager.deploy_pipeline()
    
    # Execute pipeline
    execution = pipeline_manager.execute_pipeline()
    
    print(f"ðŸš€ Pipeline deployed and executed!")
    print(f"ðŸ“Š Monitor execution: https://console.aws.amazon.com/sagemaker/home#/pipelines")
    
    return pipeline, execution


if __name__ == "__main__":
    main()
