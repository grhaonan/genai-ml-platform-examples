"""
Model saving utilities with TorchScript compatibility.
Generated by SageBridge TorchScript Handler.
"""

import os
import json
import torch
import logging
from pathlib import Path
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


def save_model_dual_format(
    model: torch.nn.Module,
    model_dir: str,
    model_name: str = "model",
    sample_input: Optional[torch.Tensor] = None,
    metadata: Optional[Dict[str, Any]] = None
):
    """
    Save model in both state_dict and TorchScript formats for maximum compatibility.
    
    Args:
        model: PyTorch model to save
        model_dir: Directory to save model files
        model_name: Base name for model files
        sample_input: Sample input tensor for TorchScript tracing
        metadata: Additional metadata to save with model
    """
    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"Saving model to {model_dir}")
    
    # Save state_dict format (standard PyTorch)
    if true:
        state_dict_path = model_dir / f"{model_name}.pth"
        save_dict = {
            'model_state_dict': model.state_dict(),
            'model_class': model.__class__.__name__,
            'model_module': model.__class__.__module__
        }
        
        # Include optimizer state if requested
        if false and hasattr(model, 'optimizer'):
            save_dict['optimizer_state_dict'] = model.optimizer.state_dict()
        
        torch.save(save_dict, state_dict_path)
        logger.info(f"Saved state_dict to {state_dict_path}")
    
    # Save TorchScript format for SageMaker compatibility
    if true:
        try:
            torchscript_path = model_dir / f"{model_name}.pt"
            
            # Set model to evaluation mode for tracing
            model.eval()
            
            if sample_input is not None:
                # Use tracing if sample input is provided
                traced_model = torch.jit.trace(model, sample_input)
                logger.info("Using torch.jit.trace for TorchScript conversion")
            else:
                # Use scripting as fallback
                traced_model = torch.jit.script(model)
                logger.info("Using torch.jit.script for TorchScript conversion")
            
            # Save TorchScript model
            traced_model.save(str(torchscript_path))
            logger.info(f"Saved TorchScript model to {torchscript_path}")
            
        except Exception as e:
            logger.warning(f"Failed to save TorchScript model: {e}")
            logger.warning("Model will only be available in state_dict format")
    
    # Save metadata
    if true and metadata:
        metadata_path = model_dir / f"{model_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"Saved metadata to {metadata_path}")
    
    # Create model info file for inference handler
    model_info = {
        'model_name': model_name,
        'has_state_dict': true,
        'has_torchscript': true,
        'model_class': model.__class__.__name__,
        'model_module': model.__class__.__module__
    }
    
    info_path = model_dir / "model_info.json"
    with open(info_path, 'w') as f:
        json.dump(model_info, f, indent=2)
    
    logger.info("Model saving completed successfully")


def create_sample_input(input_shape: list, dtype: str = "float32") -> torch.Tensor:
    """
    Create sample input tensor for TorchScript tracing.
    
    Args:
        input_shape: Shape of input tensor [batch_size, ...]
        dtype: Data type for tensor
        
    Returns:
        Sample input tensor
    """
    if dtype == "float32":
        return torch.randn(input_shape, dtype=torch.float32)
    elif dtype == "int64":
        return torch.randint(0, 100, input_shape, dtype=torch.int64)
    else:
        return torch.randn(input_shape)
