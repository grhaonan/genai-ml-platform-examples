"""
SageMaker Inference Handler with TorchScript compatibility.
Generated by SageBridge TorchScript Handler.
"""

import os
import json
import torch
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Union, Optional

logger = logging.getLogger(__name__)


class ModelHandler:
    """
    SageMaker inference handler with dual model loading support.
    
    Supports both TorchScript and state_dict model formats with automatic fallback.
    """
    
    def __init__(self):
        self.model = None
        self.device = None
        self.model_info = None
        self.initialized = False
        
    def initialize(self, context):
        """
        Initialize the model handler.
        
        Args:
            context: SageMaker inference context
        """
        try:
            # Get model directory
            model_dir = context.system_properties.get("model_dir", "/opt/ml/model")
            model_dir = Path(model_dir)
            
            logger.info(f"Initializing model from {model_dir}")
            
            # Load model info
            info_path = model_dir / "model_info.json"
            if info_path.exists():
                with open(info_path, 'r') as f:
                    self.model_info = json.load(f)
            else:
                logger.warning("model_info.json not found, using defaults")
                self.model_info = {"has_torchscript": True, "has_state_dict": True}
            
            # Set device
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"Using device: {self.device}")
            
            # Load model with fallback mechanism
            self.model = self._load_model_with_fallback(model_dir)
            
            if self.model is None:
                raise RuntimeError("Failed to load model in any format")
            
            # Set model to evaluation mode
            self.model.eval()
            self.model.to(self.device)
            
            self.initialized = True
            logger.info("Model initialization completed successfully")
            
        except Exception as e:
            logger.error(f"Model initialization failed: {e}")
            raise
    
    def _load_model_with_fallback(self, model_dir: Path) -> Optional[torch.nn.Module]:
        """
        Load model with fallback mechanism: TorchScript -> state_dict.
        
        Args:
            model_dir: Directory containing model files
            
        Returns:
            Loaded PyTorch model
        """
        model = None
        
        # Try TorchScript first (preferred for SageMaker)
        if self.model_info.get("has_torchscript", False):
            torchscript_files = list(model_dir.glob("*.pt"))
            for pt_file in torchscript_files:
                try:
                    logger.info(f"Attempting to load TorchScript model from {pt_file}")
                    model = torch.jit.load(str(pt_file), map_location=self.device)
                    logger.info("Successfully loaded TorchScript model")
                    return model
                except Exception as e:
                    logger.warning(f"Failed to load TorchScript model from {pt_file}: {e}")
        
        # Fallback to state_dict format
        if self.model_info.get("has_state_dict", False):
            state_dict_files = list(model_dir.glob("*.pth"))
            for pth_file in state_dict_files:
                try:
                    logger.info(f"Attempting to load state_dict model from {pth_file}")
                    
                    # Load checkpoint
                    checkpoint = torch.load(str(pth_file), map_location=self.device)
                    
                    # This requires the model class to be available
                    # In practice, this would need to be customized based on the specific model
                    logger.warning("state_dict loading requires model class definition")
                    logger.warning("Consider using TorchScript format for better compatibility")
                    
                    # Placeholder for model instantiation
                    # model = ModelClass()  # This needs to be customized
                    # model.load_state_dict(checkpoint['model_state_dict'])
                    
                except Exception as e:
                    logger.warning(f"Failed to load state_dict model from {pth_file}: {e}")
        
        return model
    
    def preprocess(self, data: Any) -> torch.Tensor:
        """
        Preprocess input data for model inference.
        
        Args:
            data: Raw input data
            
        Returns:
            Preprocessed tensor
        """
        try:
            # Handle different input formats
            if isinstance(data, dict):
                # JSON input
                if "instances" in data:
                    inputs = data["instances"]
                elif "inputs" in data:
                    inputs = data["inputs"]
                else:
                    inputs = data
            else:
                inputs = data
            
            # Convert to tensor
            if isinstance(inputs, (list, np.ndarray)):
                tensor = torch.tensor(inputs, dtype=torch.float32)
            elif isinstance(inputs, torch.Tensor):
                tensor = inputs
            else:
                raise ValueError(f"Unsupported input type: {type(inputs)}")
            
            # Ensure correct shape and device
            if tensor.dim() == 1:
                tensor = tensor.unsqueeze(0)  # Add batch dimension
            
            tensor = tensor.to(self.device)
            
            # Additional preprocessing can be added here
            if true:
                tensor = self._apply_preprocessing(tensor)
            
            return tensor
            
        except Exception as e:
            logger.error(f"Preprocessing failed: {e}")
            raise
    
    def inference(self, data: torch.Tensor) -> torch.Tensor:
        """
        Run model inference.
        
        Args:
            data: Preprocessed input tensor
            
        Returns:
            Model output tensor
        """
        try:
            with torch.no_grad():
                output = self.model(data)
            return output
            
        except Exception as e:
            logger.error(f"Inference failed: {e}")
            raise
    
    def postprocess(self, data: torch.Tensor) -> Dict[str, Any]:
        """
        Postprocess model output.
        
        Args:
            data: Model output tensor
            
        Returns:
            Formatted output
        """
        try:
            # Convert to numpy for JSON serialization
            if isinstance(data, torch.Tensor):
                data = data.cpu().numpy()
            
            # Additional postprocessing can be added here
            if true:
                data = self._apply_postprocessing(data)
            
            return {
                "predictions": data.tolist(),
                "model_info": self.model_info
            }
            
        except Exception as e:
            logger.error(f"Postprocessing failed: {e}")
            raise
    
    def _apply_preprocessing(self, tensor: torch.Tensor) -> torch.Tensor:
        """Apply custom preprocessing transformations."""
        # Placeholder for custom preprocessing
        # This would be customized based on the specific model requirements
        return tensor
    
    def _apply_postprocessing(self, data: np.ndarray) -> np.ndarray:
        """Apply custom postprocessing transformations."""
        # Placeholder for custom postprocessing
        # This would be customized based on the specific model requirements
        return data


# Global model handler instance
_model_handler = ModelHandler()


def model_fn(model_dir: str) -> ModelHandler:
    """
    Load model for SageMaker inference.
    
    Args:
        model_dir: Directory containing model artifacts
        
    Returns:
        Initialized model handler
    """
    # Create a mock context for initialization
    class MockContext:
        def __init__(self, model_dir):
            self.system_properties = {"model_dir": model_dir}
    
    context = MockContext(model_dir)
    _model_handler.initialize(context)
    return _model_handler


def input_fn(request_body: str, content_type: str = "application/json") -> Any:
    """
    Parse input data for inference.
    
    Args:
        request_body: Raw request body
        content_type: Content type of request
        
    Returns:
        Parsed input data
    """
    if content_type == "application/json":
        return json.loads(request_body)
    elif content_type == "text/csv":
        # Handle CSV input
        import io
        import pandas as pd
        return pd.read_csv(io.StringIO(request_body)).values
    else:
        raise ValueError(f"Unsupported content type: {content_type}")


def predict_fn(input_data: Any, model: ModelHandler) -> Dict[str, Any]:
    """
    Run prediction on input data.
    
    Args:
        input_data: Parsed input data
        model: Model handler instance
        
    Returns:
        Prediction results
    """
    # Preprocess
    preprocessed = model.preprocess(input_data)
    
    # Inference
    output = model.inference(preprocessed)
    
    # Postprocess
    result = model.postprocess(output)
    
    return result


def output_fn(prediction: Dict[str, Any], accept: str = "application/json") -> str:
    """
    Format prediction output.
    
    Args:
        prediction: Prediction results
        accept: Accepted response format
        
    Returns:
        Formatted response
    """
    if accept == "application/json":
        return json.dumps(prediction)
    else:
        raise ValueError(f"Unsupported accept type: {accept}")
