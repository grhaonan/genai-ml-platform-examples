# Sagemigrator SageMaker Pipeline

Generated by SageMigrator CLI on 2026-01-13 09:14:50

## Overview

This SageMaker Pipeline implements a complete MLOps workflow with four main steps:

1. **Preprocess Data**: Download, prepare, and upload training dataset to S3
2. **Train Model**: Train a machine learning model using PyTorch
3. **Evaluate Model**: Assess model performance and generate metrics
4. **Register Model**: Conditionally register the model based on evaluation results

## Pipeline Configuration

- **Role**: `arn:aws:iam::624178040188:role/sagemigrator-SageMaker-ExecutionRole-dev`
- **S3 Bucket**: `sagemigrator-sagemaker-bucket-624178040188-us-east-1`
- **Region**: `us-east-1` (default: us-east-1)
- **Accuracy Threshold**: `0.85`
- **Instance Type**: `ml.c5.xlarge`
- **Framework**: PyTorch 2.1.0

## Files

- `pipeline.py` - Main pipeline definition and execution
- `preprocessing.py` - Data preprocessing script
- `evaluation.py` - Model evaluation script
- `deploy_pipeline.py` - Pipeline deployment script
- `README.md` - This documentation

## Quick Start

### 1. Deploy the Pipeline

```bash
python deploy_pipeline.py
```

### 2. Execute the Pipeline

```bash
python pipeline.py
```

### 3. Monitor Execution

Visit the SageMaker console to monitor pipeline execution:
```
https://console.aws.amazon.com/sagemaker/home#/pipelines
```

## Pipeline Steps

### Step 1: Preprocess Data

- Downloads MNIST dataset using scikit-learn
- Normalizes pixel values to [0, 1] range
- Splits data into 80% train, 20% test with stratified sampling
- Saves data in Parquet format to S3
- Generates data quality report and metadata

**Outputs:**
- `s3://sagemigrator-sagemaker-bucket-624178040188-us-east-1/data/train/` - Training data in Parquet format
- `s3://sagemigrator-sagemaker-bucket-624178040188-us-east-1/data/test/` - Test data in Parquet format
- Quality reports and metadata files

### Step 2: Train Model

- Uses PyTorch estimator with configurable hyperparameters
- Reads preprocessed data from S3
- Supports spot instances for cost optimization
- Saves model artifacts to S3

**Parameters:**
- `Epochs`: Number of training epochs (default: 10)
- `BatchSize`: Training batch size (default: 64)
- `LearningRate`: Learning rate (default: 0.001)
- `TrainingInstanceType`: Instance type (default: ml.c5.xlarge)

### Step 3: Evaluate Model

- Loads trained model from artifacts
- Runs evaluation on test dataset
- Generates performance metrics (accuracy, precision, recall, F1)
- Saves evaluation results for conditional registration

**Metrics Generated:**
- Accuracy
- Precision
- Recall
- F1 Score
- Total samples evaluated

### Step 4: Conditional Model Registration

- Checks if model accuracy meets threshold (0.85)
- **If accuracy â‰¥ threshold**: Auto-approves and registers model
- **If accuracy < threshold**: Registers model but requires manual approval

**Model Registry:**
- Group Name: `sagemigrator-model-group`
- Approval Status: Automatic or Manual based on performance
- Includes model metrics and lineage information

## Customization

### Modify Hyperparameters

Edit the pipeline parameters in `pipeline.py`:

```python
self.epochs = ParameterInteger(name="Epochs", default_value=20)
self.batch_size = ParameterInteger(name="BatchSize", default_value=128)
self.learning_rate = ParameterFloat(name="LearningRate", default_value=0.01)
```

### Change Accuracy Threshold

Update the threshold in `pipeline.py`:

```python
self.accuracy_threshold = ParameterFloat(name="AccuracyThreshold", default_value=0.98)
```

### Modify Evaluation Logic

Edit `evaluation.py` to:
- Load your specific test dataset
- Implement custom evaluation metrics
- Add additional validation checks

## Execution with Custom Parameters

```python
from pipeline import SagemigratorPipeline

pipeline_manager = SagemigratorPipeline()
execution = pipeline_manager.execute_pipeline(parameters={
    'Epochs': 20,
    'BatchSize': 128,
    'AccuracyThreshold': 0.98
})
```

## Monitoring and Debugging

### View Pipeline Execution

```bash
aws sagemaker list-pipeline-executions \
    --pipeline-name sagemigrator-pipeline \
    --region us-east-1
```

### Check Training Logs

```bash
aws logs get-log-events \
    --log-group-name /aws/sagemaker/TrainingJobs \
    --log-stream-name <training-job-name>/algo-1-* \
    --region us-east-1
```

### View Model Registry

```bash
aws sagemaker list-model-packages \
    --model-package-group-name sagemigrator-model-group \
    --region us-east-1
```

## Troubleshooting

### IAM Role Setup

If you encounter role-related errors, you may need to create or configure a SageMaker execution role:

#### Option 1: Create Role via AWS CLI

```bash
# Create the IAM role
aws iam create-role \
    --role-name sagemigrator-SageMaker-ExecutionRole \
    --assume-role-policy-document '{
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Principal": {
                    "Service": "sagemaker.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
            }
        ]
    }'

# Attach SageMaker policy
aws iam attach-role-policy \
    --role-name sagemigrator-SageMaker-ExecutionRole \
    --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
```

#### Option 2: Use AWS Console

1. Go to [IAM Console](https://console.aws.amazon.com/iam/)
2. Create a new role with SageMaker service trust relationship
3. Attach the `AmazonSageMakerFullAccess` policy
4. Update the role ARN in your pipeline configuration

#### Option 3: Use Existing Role

If you have an existing SageMaker role, update the pipeline configuration:

```python
# In pipeline.py, modify the role parameter
pipeline_manager = SagemigratorPipeline(
    role="arn:aws:iam::YOUR-ACCOUNT:role/YOUR-EXISTING-ROLE"
)
```

### S3 Bucket Setup

If you encounter S3 bucket errors, you may need to create the bucket:

#### Option 1: Create Bucket via AWS CLI

```bash
# Create the S3 bucket
aws s3 mb s3://sagemigrator-sagemaker-bucket-624178040188-us-east-1 --region us-east-1

# Enable versioning
aws s3api put-bucket-versioning \
    --bucket sagemigrator-sagemaker-bucket-624178040188-us-east-1 \
    --versioning-configuration Status=Enabled

# Enable encryption
aws s3api put-bucket-encryption \
    --bucket sagemigrator-sagemaker-bucket-624178040188-us-east-1 \
    --server-side-encryption-configuration '{
        "Rules": [{
            "ApplyServerSideEncryptionByDefault": {
                "SSEAlgorithm": "AES256"
            }
        }]
    }'
```

#### Option 2: Use AWS Console

1. Go to [S3 Console](https://console.aws.amazon.com/s3/)
2. Create a new bucket named: `sagemigrator-sagemaker-bucket-624178040188-us-east-1`
3. Enable versioning and encryption
4. Set appropriate permissions for your SageMaker role

#### Option 3: Use Different Bucket

If you have an existing bucket, update the pipeline configuration:

```python
# In pipeline.py, modify the bucket parameter
pipeline_manager = SagemigratorPipeline(
    bucket="your-existing-bucket-name"
)
```

### Common Issues

1. **Role Not Found**: Create the SageMaker execution role (see IAM Role Setup above)
2. **Bucket Not Found**: Create the S3 bucket (see S3 Bucket Setup above)
3. **Permission Errors**: Ensure the role has `AmazonSageMakerFullAccess` policy
4. **S3 Access**: Verify bucket exists and role has read/write access
5. **Model Loading**: Check that evaluation script matches your model architecture
6. **Resource Limits**: Ensure account has sufficient service limits

### Debug Mode

Enable debug logging in the pipeline:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Cost Optimization

- **Spot Instances**: Enabled by default for training (up to 70% savings)
- **Right-sizing**: Choose appropriate instance types for your workload
- **Cleanup**: Pipeline automatically cleans up intermediate resources

## Security

- **IAM Roles**: Uses least-privilege access patterns
- **Encryption**: S3 and SageMaker encryption enabled
- **VPC**: Can be configured for network isolation

## Next Steps

1. **Customize Training**: Modify training script for your specific use case
2. **Add Data Processing**: Include data preprocessing steps
3. **Implement A/B Testing**: Add model comparison logic
4. **Set up Monitoring**: Configure CloudWatch alarms and dashboards
5. **Automate Deployment**: Integrate with CI/CD pipelines

## Support

For issues and questions:
- Check SageMaker documentation
- Review CloudWatch logs
- Validate IAM permissions
- Test with smaller datasets first

---

Generated by **SageMigrator CLI** - Intelligent EC2 to SageMaker Migration System
