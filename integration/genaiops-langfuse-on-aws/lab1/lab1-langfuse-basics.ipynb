{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Langfuse Tracing\n",
    "\n",
    "In this lab, we will learn how to use Langfuse tracing to log and analyze the execution of your LLM applications. The Langfuse supports self-hosted on AWS (this lab) and there is a [cloud version](https://cloud.langfuse.com/) available. [Tracing](https://langfuse.com/docs/tracing) in Langfuse is a way to log and analyze the execution of your LLM applications and following reference provides a detailed overview of the data model used. It is inspired by OpenTelemetry.\n",
    "\n",
    "\n",
    "## [Traces and Observations](https://langfuse.com/docs/tracing-data-model)\n",
    "A trace typically represents a single request or operation. It contains the overall input and output of the function, as well as metadata about the request, such as the user, the session, and tags. Usually, a trace corresponds to a single api call of an application.\n",
    "\n",
    "Each trace can contain multiple observations to log the individual steps of the execution.\n",
    "\n",
    "- Observations are of different types:\n",
    "    - Events are the basic building blocks. They are used to track discrete events in a trace.\n",
    "    - Spans represent durations of units of work in a trace.\n",
    "    - Generations are spans used to log generations of AI models. They contain additional attributes about the model, the prompt, and the completion. For generations, [token usage and costs](https://langfuse.com/docs/model-usage-and-cost) are automatically calculated.\n",
    "- Observations can be nested.\n",
    "\n",
    "![Trace and Observations](./images/trace-observation.png)\n",
    "![Trace and Observations UI](./images/trace-observation-ui.png)\n",
    "\n",
    "## [Sessions](https://langfuse.com/docs/tracing-data-model)\n",
    "Optionally, traces can be grouped into sessions. Sessions are used to group traces that are part of the same user interaction. A common example is a thread in a chat interface.\n",
    "Please refer to the [Sessions documentation](https://langfuse.com/docs/sessions) to add sessions to your traces.\n",
    "\n",
    "![Trace and Sessions](./images/trace-sessions.png)\n",
    "![Trace and Sessions UI](./images/trace-sessions-ui.png)\n",
    "\n",
    "\n",
    "## [Scores](https://langfuse.com/docs/tracing-data-model)\n",
    "\n",
    "Traces and observations can be evaluated using [scores](https://langfuse.com/docs/scores/overview). Scores are flexible objects that store evaluation metrics and can be:\n",
    "\n",
    "- Numeric, categorical, or boolean values\n",
    "- Associated with a trace (required)\n",
    "- Linked to a specific observation (optional)\n",
    "- Annotated with comments for additional context\n",
    "- Validated against a score configuration schema (optional)\n",
    "\n",
    "![Trace and Scores](./images/trace-scores.png)\n",
    "\n",
    "Please refer to the [scores documentation](https://langfuse.com/docs/scores/overview) to get started. For more details on score types and attributes, refer to the [score data model documentation](https://langfuse.com/docs/scores/data-model).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Scores\n",
    "\n",
    "Traces and observations can be evaluated using [scores](https://langfuse.com/docs/scores/overview). Scores are flexible objects that store evaluation metrics and can be:\n",
    "\n",
    "- Numeric, categorical, or boolean values\n",
    "- Associated with a trace (required)\n",
    "- Linked to a specific observation (optional)\n",
    "- Annotated with comments for additional context\n",
    "- Validated against a score configuration schema (optional)\n",
    "\n",
    "![Trace and Scores](./images/trace-scores.png)\n",
    "\n",
    "[Source](https://langfuse.com/docs/scores/overview)\n",
    "\n",
    "Please refer to the [scores documentation](https://langfuse.com/docs/scores/overview) to get started. For more details on score types and attributes, refer to the [score data model documentation](https://langfuse.com/docs/scores/data-model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner, select Python Environments and choose \".venv (Python 3.9.20) .venv/bin/python Recommended\".\n",
    "\n",
    "> To execute each notebook cell, press Shift + Enter.\n",
    "\n",
    "> ℹ️ You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Environment Variables\n",
    "\n",
    "We will use the langfuse and boto3:\n",
    "- The Langfuse Python SDK along with the self-hosting deployment to debug and improve LLM applications by tracing model invocations, managing prompts / models configurations and running evaluations.\n",
    "- The boto3 SDK to interact with models on Amazon Bedrock or Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you execute any cell, it will prompt to request install the extention and please select \"install/enable suggested extensions\"\n",
    "\n",
    "![Install-vscode-extension](./images/vscode-install-suggested-extension.png)\n",
    "\n",
    "Then click on \"Trust Publisher & Install\" button and installation will take a while and once it is done, it will prompt again and please select \"Python Environments\"\n",
    "\n",
    "![Install-python-extension](./images/vscode-install-python-env.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to install the required Python SDKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!uv pip install --force-reinstall -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys in the .env file to connect to self-hosted or cloud Langfuse environment.\n",
    "\n",
    "1. Navigate to the directory `genai-ml-platform-examples/integration/genaiops-langfuse-on-aws/` within your workshop environment.\n",
    "\n",
    "2. Locate the file named `.env.example` and create a copy of this file in the same directory, renaming the copy to `.env`.\n",
    "\n",
    "3. Open the `.env` file in your editor and prepare to add your actual Langfuse credentials. You will need three values from your Langfuse project settings under the API Keys section.\n",
    "\n",
    "The completed configuration in `.env` should follow this format:\n",
    "\n",
    "```\n",
    "LANGFUSE_PUBLIC_KEY=pk-lf-your-actual-public-key\n",
    "LANGFUSE_SECRET_KEY=sk-lf-your-actual-secret-key\n",
    "LANGFUSE_HOST=xxx\n",
    "```\n",
    "\n",
    "Save the file after adding your actual credential values. The notebook will load these environment variables automatically when executing the Langfuse integration exercises in agents two and three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you completed the .env setup above, skip this cell.\n",
    "# Otherwise, uncomment and set your Langfuse credentials below:\n",
    "\n",
    "# import os\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"  # Your Langfuse project secret key\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"  # Your Langfuse project public key\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"xxx\"  # Your Langfuse host URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check\n",
    "Run the following cells to initialize common libraries and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from langfuse import Langfuse\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from langfuse.model import PromptClient\n",
    "\n",
    "\n",
    "# Load environment variables from ../.env file\n",
    "# This is the preferred and safer way to manage credentials\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create bedrock client and bedrock runtime client and please make sure the region is us-west-2 for this lab. The expected result is to see the following output:\n",
    "\n",
    "```\n",
    "Found Nova model: US Nova Pro - us.amazon.nova-pro-v1:0\n",
    "Found Nova model: US Nova Lite - us.amazon.nova-lite-v1:0\n",
    "Found Nova model: US Nova Micro - us.amazon.nova-micro-v1:0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> As Nova models in us-west-2 can only be called via Cross-Region Inference (CRIS), the model_id has \"us.\" prefix to indicate this is a CRIS call. This can add latency to the model call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to access Bedrock configuration\n",
    "# region has to be in us-west-2 for this lab\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "\n",
    "# Check if Nova models are available in this region\n",
    "models = bedrock.list_inference_profiles()\n",
    "nova_found = False\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    if (\n",
    "        \"Nova Pro\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Lite\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Micro\" in model[\"inferenceProfileName\"]\n",
    "    ):\n",
    "        print(f\"Found Nova model: {model['inferenceProfileName']} - {model['inferenceProfileId']}\")\n",
    "        nova_found = True\n",
    "if not nova_found:\n",
    "    raise ValueError(\"No Nova models found in available models. Please ensure you have access to Nova models.\")\n",
    "#  Coverage, log level, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse Wrappers for Bedrock Converse API \n",
    "You can use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an Amazon Bedrock model. For example, you can create a chat bot that maintains a conversation over many turns and uses a persona or tone customization that is unique to your needs, such as a helpful technical support assistant.\n",
    "\n",
    "To use the Converse API, you use the Converse or ConverseStream (for streaming responses) operations to send messages to a model. It is possible to use the existing base inference operations (InvokeModel or InvokeModelWithResponseStream) for conversation applications. However, we recommend using the Converse API as it provides consistent API, that works with all Amazon Bedrock models that support messages. This means you can write code once and use it with different models. Should a model have unique inference parameters, the Converse API also allows you to pass those unique parameters in a model specific structure.\n",
    "\n",
    "For more details, please refer to the [Carry out a conversation with the Converse API operations](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"..\"))  # Add parent directory to path\n",
    "from config import GUARDRAIL_CONFIG, MODEL_CONFIG\n",
    "from utils import converse, converse_tool_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Examples\n",
    "\n",
    "#### Define a helper function to call the  Converse API wrapper\n",
    "\n",
    "> Please make sure your have setup the Nova custom model pricing per mentioned in  [Langfuse Setup](https://catalog.workshops.aws/genaiops-langfuse/en-US/00-introduction/langfuse-setup) under Introduction section of the workshop studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Simple Chat\")\n",
    "def simple_chat(\n",
    "    model_config: dict,\n",
    "    messages: list,\n",
    "    prompt: PromptClient = None,\n",
    "    use_guardrails: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Executes a simple chat interaction using the specified model configuration.\n",
    "\n",
    "    Args:\n",
    "        model_config (dict): Configuration parameters for the chat model.\n",
    "        messages (list): A list of message dictionaries to be processed.\n",
    "        prompt (PromptClient, optional): Optional prompt client for advanced handling.\n",
    "        use_guardrails (bool, optional): When True, applies additional guardrail configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the 'converse' function call.\n",
    "    \"\"\"\n",
    "    config = model_config.copy()\n",
    "    if use_guardrails:\n",
    "        config[\"guardrailConfig\"] = GUARDRAIL_CONFIG\n",
    "    return converse(messages=messages, prompt=prompt, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 1\n",
    "let's start with a single turn chat use case and use Nova Pro as the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to observe and track this function execution in Langfuse\n",
    "@observe(name=\"Single Turn Example\")\n",
    "def chat_single_model(messages: list, model_type: str = \"nova_pro\", use_guardrails: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Execute a single turn chat interaction using one specified Nova model.\n",
    "\n",
    "    Args:\n",
    "        messages (list): The user's input query\n",
    "        model_type (str): The Nova model to use (nova_pro, nova_lite, or nova_micro)\n",
    "        use_guardrails (bool): Whether to apply guardrails to the model invocation\n",
    "\n",
    "    Returns:\n",
    "        dict: Response containing model output and status code\n",
    "    \"\"\"\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        tags=[\"lab1\", \"single-turn\"],\n",
    "    )\n",
    "\n",
    "    response = simple_chat(\n",
    "        model_config=MODEL_CONFIG[model_type],\n",
    "        messages=messages,\n",
    "        use_guardrails=use_guardrails,\n",
    "    )\n",
    "\n",
    "    return {\"model\": model_type, \"response\": response, \"statusCode\": 200}\n",
    "\n",
    "\n",
    "# Make a sample request to test the chat API\n",
    "# Ask about luxury resort check-in process\n",
    "print(\n",
    "    chat_single_model(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain the process of checking in a guest at a luxury resort, think step by step.\",\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Force immediate sending of the trace data to Langfuse\n",
    "# Rather than waiting for automatic flush\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"In Langfuse dashboard, you can find the summary of the traces, model costs and model usage.:\\n{os.environ['LANGFUSE_HOST']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Langfuse Dashboard](./images/langfuse-dashboard-use-case-1.png)\n",
    "\n",
    "\n",
    "The detailed traced can be found in the **Traces** section and you can click on the trace to see the detailed trace. And there are some key insights in this trace.\n",
    "\n",
    "- input token is 12 and output token is 662, in total there are 675 tokens used\n",
    "\n",
    "- The model used is us.amazon.nova-pro-v1:0\n",
    "\n",
    "- Model parameters are shown such as temperature, max_tokens, etc.\n",
    "\n",
    "- Most importantly, the total cost of this invovation costs $0.002129 \n",
    "\n",
    "![Langfuse Dashboard](./images/langfuse-trace-use-case-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 2\n",
    "This use case demonstrates running a single trace within one session, where we'll execute three distinct observations using different Nova model variants for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Multi-Turn Example\")\n",
    "def chat_compare_models(\n",
    "    messages: list,\n",
    "    model_types: list = [\"nova_pro\", \"nova_lite\", \"nova_micro\"],\n",
    "    use_guardrails: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Execute the same query across all Nova models for comparison.\n",
    "\n",
    "    Args:\n",
    "        messages (list): The user's input query\n",
    "        model_types (list): The Nova models to use (nova_pro, nova_lite, or nova_micro)\n",
    "        use_guardrails (bool): Whether to apply guardrails to the model invocation\n",
    "    Returns:\n",
    "        dict: Responses from all models and status code\n",
    "    \"\"\"\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"model-comparison\",\n",
    "        tags=[\"lab1\", \"model-comparison\"],\n",
    "    )\n",
    "\n",
    "    responses = {}\n",
    "    for model_type in model_types:\n",
    "        responses[model_type] = simple_chat(\n",
    "            model_config=MODEL_CONFIG[model_type],\n",
    "            messages=messages,\n",
    "            use_guardrails=use_guardrails,\n",
    "        )\n",
    "\n",
    "    return {\"responses\": responses, \"statusCode\": 200}\n",
    "\n",
    "\n",
    "# user request\n",
    "print(\n",
    "    chat_compare_models(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain the process of checking in a guest at a luxury resort, think step by step.\",\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can combine multiple observations into one trace and see the cost and usage of each observation. Nova micro has the lowest cost and fastest response time.\n",
    "\n",
    "![langfuse-traces-use-case-2](./images/langfuse-trace-use-case-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 3\n",
    "In this case, let's simulate a RAG use case with a dummy retrieval function called retrieve_context, it is a dummy function that returns a static context.\n",
    "We will simply reuse the simple_chat function to chat with the model by passing the context from the retrieval function as part of the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"1st January 2025\n",
    "Sydney: 24 degrees celcius.\n",
    "New York: 13 degrees celcius.\n",
    "Tokyo: 11 degrees celcius.\"\"\"\n",
    "\n",
    "\n",
    "@observe(name=\"Dummy Retrieval\")\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"Retrieves static context for the given query.\"\"\"\n",
    "    return CONTEXT\n",
    "\n",
    "\n",
    "@observe(name=\"RAG Example\")\n",
    "def rag_api(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Performs a Retrieval-Augmented Generation (RAG) query using a static context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the model and a status code.\n",
    "    \"\"\"\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"rag-session\",\n",
    "        tags=[\"lab1\", \"rag-example\"],\n",
    "    )\n",
    "\n",
    "    context = retrieve_context(query)\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Context: {context}\\nBased on the context above, answer the following question:\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": query, \"role\": \"user\"},\n",
    "    ]\n",
    "    response = simple_chat(model_config=MODEL_CONFIG[\"nova_pro\"], messages=messages)\n",
    "\n",
    "    return {\"response\": response, \"statusCode\": 200}\n",
    "\n",
    "\n",
    "# User request\n",
    "print(rag_api(\"how you like the weather in Sydney? any comments?\"))\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trace, you can see the retrieval function is called and the context is passed to the model as part of the system prompt. The fine model invocation takes both system prompt and user prompt and return the response.\n",
    "\n",
    "![langfuse-traces-use-case-3](./images/langfuse-trace-use-case-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 4\n",
    "\n",
    "Multi-Modal Capabilities with Image Support\n",
    "\n",
    "Modern AI systems are increasingly adopting multi-modal capabilities, allowing them to process and understand different types of data inputs, including text, images, and audio. In this example, we demonstrate how Langfuse supports tracing for image-based inputs, which is particularly valuable for:\n",
    "\n",
    "1. **Image Analysis**: Interpreting and describing visual content\n",
    "2. **Visual Question Answering**: Answering questions based on image context\n",
    "3. **Document Processing**: Extracting information from scanned documents or images\n",
    "4. **Content Moderation**: Identifying inappropriate or sensitive visual content\n",
    "\n",
    "The implementation uses a structured message format where the image URL is passed as part of the user prompt, enabling the model to process both textual queries and visual information simultaneously. This capability is especially useful in applications like:\n",
    "\n",
    "- E-commerce product recognition\n",
    "- Medical image analysis\n",
    "- Social media content understanding\n",
    "\n",
    "Langfuse's tracing capabilities extend to these multi-modal interactions, providing visibility into how the model processes and responds to image inputs, which is crucial for debugging and improving these complex systems.\n",
    "\n",
    "In this use case, we will pass images as part of the user prompt and the model will process both the text and the image and langfuse will trace the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Multi-Modal Image Example\")\n",
    "def vision_api(\n",
    "    query: str,\n",
    "    image_url: str,\n",
    ") -> str | None:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"vision-session\",\n",
    "        tags=[\"lab1\", \"vision-example\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI trained to describe and interpret images.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": simple_chat(model_config=MODEL_CONFIG[\"nova_pro\"], messages=messages),\n",
    "        \"statusCode\": 200,\n",
    "    }\n",
    "\n",
    "\n",
    "# image source: https://www.aboutamazon.com/news/aws/aws-reinvent-2024-keynote-live-news-updates\n",
    "print(\n",
    "    vision_api(\n",
    "        query=\"What is happening in this image?\",\n",
    "        image_url=\"https://amazon-blogs-brightspot.s3.amazonaws.com/df/82/368cb270402e9739f04905ea9b19/swami-bedrock.jpeg\",\n",
    "    )\n",
    ")\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langfuse also supports tracing with image input, this is very useful for a multi-modal use case in which the model can take image as input.\n",
    "\n",
    "![langfuse-traces-use-case-4](./images/langfuse-trace-use-case-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Use with Langfuse Tracing\n",
    "Tool use enables AI models to interact with external functions and APIs, extending their capabilities beyond pure text generation. This is particularly useful for:\n",
    "- Accessing real-time data (e.g., weather, stock prices)\n",
    "- Performing complex calculations\n",
    "- Integrating with external systems\n",
    "- Extracting structured data from unstructured sources (e.g. text, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 1\n",
    "\n",
    "The example below demonstrates a weather information tool implementation. When a user asks about weather conditions, the model will:\n",
    "\n",
    "1. Recognize the need for weather data\n",
    "2. Extract location/unit parameters through structured tool definition\n",
    "3. Return a formatted response using our `get_current_weather` tool\n",
    "\n",
    "Expected result: The model should identify San Francisco as the location and celsius as the preferred unit, returning a structured tool response while maintaining full trace visibility in Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Tool Use Example\")\n",
    "def tool_use_api(query: str) -> list:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"tool-use-session\",\n",
    "        tags=[\"lab1\", \"tool-use\"],\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": converse_tool_use(messages, tools, tool_choice=\"auto\", **MODEL_CONFIG[\"nova_pro\"]),\n",
    "        \"statusCode\": 200,\n",
    "    }\n",
    "\n",
    "\n",
    "print(tool_use_api(query=\"What's the weather like in San Francisco?, in celsius?\"))\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langfuse-traces-tool-use](./images/langfuse-trace-tool-use.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2\n",
    "The example below demonstrates a multi-modal document transcription tool implementation. When a user asks to transcribe a document, the model will:\n",
    "\n",
    "1. Recognize the schema for invoice transcription\n",
    "2. Extract structured data from images through structured tool definition\n",
    "3. Apply dependentSchemas to provide additional classification and reasoning for the extraction\n",
    "\n",
    "Expected result: The model should extract all the metadata and line items from the invoice and output the structured data in JSON format with classification and reasoning for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "<instructions>\n",
    "  - Ensure to escape quotes in the JSON response\n",
    "  - Return \"\" for missing field values\n",
    "  - Apply dependentSchemas to all <document/> fields\n",
    "</instructions>\n",
    "\n",
    "<document>\n",
    "{\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"$id\": \"/schemas/document\",\n",
    "    \"type\": \"object\",\n",
    "    \"description\": \"A document with the fields to transcribe\",\n",
    "    \"properties\": {\n",
    "        \"doc_type\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Type of Document: Receipt\" },\n",
    "        \"receipt_number\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"The receipt number or other identifier number\" },\n",
    "        \"doc_amount_total\": { \"properties\":{\"value\":{\"type\":\"number\"}}, \"description\": \"The total receipt amount\" },\n",
    "        \"currency\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"AUD/USD/CAD\" },\n",
    "        \"vendor_business_number\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Vendor's business identification number e.g. ABN\" },\n",
    "        \"vendor_name\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Business name issueing the receipt\" },\n",
    "        \"vendor_address\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Vendor's site address\" },\n",
    "        \"vendor_phone\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"Vendor's phone number\" },\n",
    "        \"payment_method\": { \"properties\":{\"value\":{\"type\":\"string\"}}, \"description\": \"The payment type, e.g. EFTPOS, Card\" },\n",
    "        \"date_issued\": { \"properties\":{\"value\":{\"format\": \"YYYY-MM-DDThh:mm:ss\"}}, \"description\": \"Date document was issued\"},\n",
    "        \"line_items_amount_total\": { \"properties\":{\"value\":{\"type\":\"number\"}}, \"description\": \"Calculated sum of line item's line_amount fields\" }\n",
    "    },\n",
    "    \"dependentSchemas\": {\n",
    "        \"value\": {\n",
    "            \"properties\": {\n",
    "                \"inference\": { \"type\": \"integer\", \"description\": \"0=EXPLICIT|1=DERIVED|2=MISSING|3=OTHER\" },\n",
    "                \"source\": { \"type\": \"string\", \"description\": \"Source locations in the document for explicit and derived fields\" }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "<document/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@observe(name=\"Vision Tool Use Example\")\n",
    "def vision_tool_use_api(\n",
    "    query: str,\n",
    "    image_url: str,\n",
    ") -> list:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"tool-use-session\",\n",
    "        tags=[\"lab1\", \"tool-use\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"transcribe_documents\",\n",
    "                \"description\": \"Extract all <document/> fields with the highest accuracy following <instructions/>\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"documents\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\"$ref\": \"/schemas/document\"},\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"documents\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"response\": converse_tool_use(messages, tools, tool_choice=\"auto\", **MODEL_CONFIG[\"nova_pro\"]),\n",
    "        \"statusCode\": 200,\n",
    "    }\n",
    "\n",
    "\n",
    "# image source: https://aws.amazon.com/blogs/machine-learning/announcing-expanded-support-for-extracting-data-from-invoices-and-receipts-using-amazon-textract/\n",
    "print(\n",
    "    vision_tool_use_api(\n",
    "        query=\"Transcribe the invoice. Make sure to apply dependentSchemas to all <document/> fields\",\n",
    "        image_url=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/07/22/ml3911-img17.jpg\",\n",
    "    )\n",
    ")\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./images/langfuse-trace-tool-use-vision.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Management\n",
    "### What is prompt management?\n",
    "\n",
    "Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications. Key aspects of prompt management include version control, decoupling prompts from code, monitoring, logging and optimizing prompts as well as integrating prompts with the rest of your application and tool stack.\n",
    "\n",
    "Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is a Prompt **CMS** (Content Management System).\n",
    "\n",
    "\n",
    "### Why use prompt management?\n",
    "\n",
    "Typical benefits of using a CMS apply here:\n",
    "\n",
    "- Decoupling: deploy new prompts without redeploying your application.\n",
    "- Non-technical users can create and update prompts via Langfuse Console.\n",
    "- Quickly rollback to a previous version of a prompt.\n",
    "- Compare different prompt versions side-by-side.\n",
    "\n",
    "Platform benefits:\n",
    "\n",
    "- Track performance of prompt versions in Langfuse Tracing.\n",
    "- Performance benefits compared to other implementations:\n",
    "\n",
    "-  No latency impact after first use of a prompt due to client-side caching and asynchronous cache refreshing.\n",
    "-  Support for text and chat prompts.\n",
    "-  Edit/manage via UI, SDKs, or API.\n",
    "\n",
    "\n",
    "There are several ways you can create prompts in Langfuse:\n",
    "\n",
    "-  Langfuse Console\n",
    "-  Langfuse SDK\n",
    "-  Langfuse API\n",
    "\n",
    "In this workshop, we will be using Langfuse Python low-level SDK to create prompts by reusing the prompt exampels from the Modul1 - Prompt Engineering with Amazon Bedrock and Nova Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Langfuse client\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Create a chat prompt without COT\n",
    "langfuse.create_prompt(\n",
    "    name=\"software-development-project-management-without-COT\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a project manager for a small software development team tasked with launching a new app feature. You want to streamline the development process and ensure timely delivery.\",\n",
    "        }\n",
    "    ],\n",
    "    labels=[\"dev\"],\n",
    "    config={\n",
    "        \"model\": MODEL_CONFIG[\"nova_pro\"][\"model_id\"],\n",
    "        \"maxTokens\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"maxTokens\"],\n",
    "        \"temperature\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"temperature\"],\n",
    "    },  # for Dev and experiment phase\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt with COT\n",
    "langfuse.create_prompt(\n",
    "    name=\"software-development-project-management-with-COT\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"You are a project manager for a small software development team tasked with launching a new app feature. You want to streamline the development process and ensure timely delivery. Please follow these steps:\\n\n",
    "       {{step1}}\\n\n",
    "       \\n\n",
    "       {{step2}}\\n\n",
    "       \\n\n",
    "       {{step3}}\\n\n",
    "       \\n\n",
    "       {{step4}}\\n\"\"\",\n",
    "        }\n",
    "    ],\n",
    "    labels=[\"dev\"],\n",
    "    config={\n",
    "        \"model\": MODEL_CONFIG[\"nova_pro\"][\"model_id\"],\n",
    "        \"maxTokens\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"maxTokens\"],\n",
    "        \"temperature\": MODEL_CONFIG[\"nova_pro\"][\"inferenceConfig\"][\"temperature\"],\n",
    "    },  # for Dev and experiment phase\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the two langfuse prompts are created successfully.\n",
    "\n",
    "![langfuse-traces-prompt-management](./images/langfuse-prompt-management.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fetch both prompts and fill in the values for the variables and call the prompts\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Get current latest version of a prompt\n",
    "sdpm_with_cot_prompt = langfuse.get_prompt(\n",
    "    \"software-development-project-management-with-COT\", type=\"chat\", label=\"dev\"\n",
    ")\n",
    "# Insert variables into prompt template\n",
    "sdpm_with_cot_prompt_compiled = sdpm_with_cot_prompt.compile(\n",
    "    step1=\"Define Requirements\",\n",
    "    step2=\"Breakdown into Tasks\",\n",
    "    step3=\"Set Deadlines\",\n",
    "    step4=\"Monitor Progress and Optimize\",\n",
    ")\n",
    "\n",
    "sdpm_without_cot_prompt = langfuse.get_prompt(\n",
    "    \"software-development-project-management-without-COT\", type=\"chat\", label=\"dev\"\n",
    ")\n",
    "sdpm_without_cot_prompt_compiled = sdpm_without_cot_prompt.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdpm_with_cot_prompt_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can add the prompt object to the generation call in the SDKs to link the generation in Langfuse Tracing to the prompt version. This linkage enables tracking of metrics by prompt version and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converesation according to AWS spec including prompting + history\n",
    "@observe()\n",
    "def main():\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"prompt-management-trace\",\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"link-prompt-session\",\n",
    "        tags=[\"lab1\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": sdpm_with_cot_prompt_compiled[0][\"role\"],\n",
    "            \"content\": sdpm_with_cot_prompt_compiled[0][\"content\"],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    simple_chat(\n",
    "        model_config=MODEL_CONFIG[\"nova_pro\"],\n",
    "        messages=messages,\n",
    "        prompt=sdpm_with_cot_prompt,\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": sdpm_without_cot_prompt_compiled[0][\"role\"],\n",
    "            \"content\": sdpm_without_cot_prompt_compiled[0][\"content\"],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    simple_chat(\n",
    "        model_config=MODEL_CONFIG[\"nova_pro\"],\n",
    "        messages=messages,\n",
    "        prompt=sdpm_without_cot_prompt,\n",
    "    )\n",
    "\n",
    "\n",
    "main()\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the trace is linked to the prompt version and the prompt name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langfuse-traces-prompt-management](./images/langfuse-link-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab1 Summary:\n",
    "In Lab1, we explored the basics of integrating Langfuse with AWS to manage prompt traces effectively.\n",
    "We demonstrated how to update traces, link prompts to user sessions, and visualize these linkages with a practical example.\n",
    "\n",
    "As we conclude this lab, take a moment to reflect on the foundational skills you've gained.\n",
    "Now, if you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will dive deeper into RAG related tracing and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
