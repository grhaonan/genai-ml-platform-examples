{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d84d252-cbca-499a-b1f3-725a6abf08e0",
   "metadata": {},
   "source": [
    "# NVIDIA Parakeet on SageMaker Asynchronous Endpoint with LMI container\n",
    "\n",
    "We can create a Python file that defines the logic for hosting NVIDIA Parakeet on the DJLServing container. We then deploy this to a SageMaker Asynchronous Endpoint allowing payloads up to 1GB.\n",
    "\n",
    "Since our code relies on ffmpeg, we need to adapt the LMI container to install `ffmpeg`. We can see this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fa7fe-9da0-4dca-95d8-62fe81a9cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\n",
    "RUN apt-get update && apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdedf0cd-e2b1-4607-beac-bf9fc452b759",
   "metadata": {},
   "source": [
    "We then push the container to ECR. \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
    "\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <ACCOUNT ID>.dkr.ecr.us-east-1.amazonaws.com\n",
    "\n",
    "docker build -t nemo-asr .\n",
    "\n",
    "docker tag nemo-asr <ACCOUNT ID>.dkr.ecr.us-east-1.amazonaws.com/nemo-asr:latest\n",
    "\n",
    "docker push <ACCOUNT ID>.dkr.ecr.us-east-1.amazonaws.com/nemo-asr:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec8106-42cf-41ff-9edf-ed6348d7de81",
   "metadata": {},
   "source": [
    "## Custom code setup\n",
    "We then write our code to a folder which we upload to S3. When the endpoint starts, it will download and use these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c2a09-5bff-4c28-b7e6-cb49fcc01a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d4f85-d795-41a2-9100-248836eda4b4",
   "metadata": {},
   "source": [
    "The `NemoASRService` defines the model loading and inference logic. It extends the built-in [HuggingFaceService](https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/huggingface.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989c686-bac0-45b1-acfd-cf3aba9f19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/NemoService.py\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import numpy as np\n",
    "\n",
    "from djl_python import Input, Output\n",
    "from djl_python.huggingface import HuggingFaceService, get_rolling_batch_class_from_str\n",
    "from djl_python.properties_manager.properties import is_rolling_batch_enabled\n",
    "from djl_python.properties_manager.hf_properties import HuggingFaceProperties\n",
    "from transformers.pipelines.audio_utils import ffmpeg_read\n",
    "\n",
    "\n",
    "class NemoASRService(HuggingFaceService):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def initialize(self, properties: dict):\n",
    "        self.hf_configs = HuggingFaceProperties(**properties)\n",
    "        self.model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\").to('cuda')\n",
    "        self.model.change_attention_model(\"rel_pos_local_attn\", [128, 128])  # local attn\n",
    "        self.model.change_subsampling_conv_chunking_factor(1)  # 1 = auto select\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.input_format_args = self.get_input_format_args()\n",
    "        self.hf_pipeline = self.call_model\n",
    "        self.initialized=True\n",
    "        \n",
    "\n",
    "    def call_model(self, input_data, **kwargs):\n",
    "        if type(input_data) is list:\n",
    "            if type(input_data[0]) is list:\n",
    "                input_data = np.array(input_data)\n",
    "            else:\n",
    "                input_list = []\n",
    "                for data in input_data:\n",
    "                    input_list.append(ffmpeg_read(data, 16000))\n",
    "                input_data = input_list\n",
    "        output = self.model.transcribe(input_data, **kwargs)\n",
    "\n",
    "        texts = []\n",
    "        for out in output:\n",
    "            texts.append(out.text)\n",
    "        return texts\n",
    "\n",
    "    def _read_model_config(self):\n",
    "        return self.model.config\n",
    "\n",
    "_service = NemoASRService()\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798140b-9391-4c02-958f-6d8b2f7bc36d",
   "metadata": {},
   "source": [
    "We disable rolling batch and set our entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42c94e-104d-4270-8996-15ecc06d24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/serving.properties\n",
    "option.rolling_batch=disable\n",
    "engine=Python\n",
    "option.entryPoint=NemoService.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc7667-2848-437e-9501-8aecee33806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "nemo_toolkit[asr]\n",
    "cuda-python\n",
    "ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d246e-27c0-45f8-80a7-16f8b7e731b0",
   "metadata": {},
   "source": [
    "## Prepare and deploy the model artifacts\n",
    "\n",
    "Once our files are prepared, we can prepare the artifacts to be deployed. This includes uploading our files to S3 then configuring and deploying the async endpoint.\n",
    "\n",
    "We configure our Model to use uncompressed artifacts as this allows for faster startup time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e125dd-f5e3-44c9-a75b-6207b2e2878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Define serializers and deserializer\n",
    "audio_serializer = DataSerializer(content_type=\"audio/x-audio\")\n",
    "deserializer = JSONDeserializer()\n",
    "# Basic configurations\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'parakeet-asr'\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_model_prefix = (\n",
    "    \"hf-asr-models/nvidia-asr\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "# below boto3 clients are for invoking asynchronous endpoint \n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91559b-047d-449e-9284-3aa27cb7b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp code/ s3://$bucket/$prefix --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65afc1-8a70-4561-8859-f887e12a49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=f\"{sess.account_id()}.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nim-sagemaker-asr:latest\"\n",
    "\n",
    "parakeet_model = Model(\n",
    "    model_data={\n",
    "        \"S3DataSource\": { \n",
    "               \"CompressionType\": \"None\",\n",
    "               \"S3DataType\": \"S3Prefix\",\n",
    "               \"S3Uri\": f\"s3://{bucket}/{prefix}/\"\n",
    "        }\n",
    "    },\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    name=name_from_base('parakeet-asr-model')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96e3a7-b78b-4c3c-9a37-2de9d2ca45bb",
   "metadata": {},
   "source": [
    "## Deploy Realtime Endpoint\n",
    "\n",
    "Deploy the model as a real-time inference endpoint. Note that if you choose to use the local SageMaker Session when creating the model object, change the instance_type to local_gpu to be able to quickly test the endpoint from local SageMaker notebook instance for fast testing. If you are going to deploy the model to an async endpoint, please make sure you create the Model object with the actual sagemaker session. In this case, it will be sess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87bda7-b206-4ebe-8637-62fedf4dcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "\n",
    "endpoint_name=name_from_base('parakeet-asr-endpoint')\n",
    "\n",
    "parakeet_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type ='ml.g5.xlarge', # instance type\n",
    "    endpoint_name = endpoint_name\n",
    ")\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=audio_serializer,\n",
    "    deserializer=deserializer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2af39-fbdb-4725-b7c0-12a72bb6db25",
   "metadata": {},
   "source": [
    "## Invoke the realtime endpoint\n",
    "\n",
    "Test the deployed real-time endpoint with a sample audio file:\n",
    "\n",
    "    Input: Audio file path (automatically serialized)\n",
    "    Processing: Synchronous transcription\n",
    "    Output: JSON response with transcription results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b386a-c868-47e3-ab89-657dae0a572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Perform real-time inference\n",
    "audio_path = \"../data/test.wav\"\n",
    "response = predictor.predict(data=audio_path)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56583f22-7583-450c-9186-d377c596994c",
   "metadata": {},
   "source": [
    "## Delete the endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ccaf2-db7c-4453-b26e-25badcc8dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba52a9-7879-4678-9589-3e6672749d14",
   "metadata": {},
   "source": [
    "## Asynchronous Inference Deployment\n",
    "\n",
    "Set up asynchronous inference config. This includes:\n",
    "- Output path in S3 for transcription results\n",
    "- Concurrency\n",
    "- SNS notifications for completion\n",
    "- Failure path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d58fa-1fec-4634-9bb2-60c834599dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_client = boto3.client('sns')\n",
    "\n",
    "def create_sns_topic_if_not_exists(topic_name, description):\n",
    "    \"\"\"Create SNS topic if it doesn't exist, return the ARN\"\"\"\n",
    "    try:\n",
    "        # Try to create the topic (idempotent operation)\n",
    "        response = sns_client.create_topic(Name=topic_name)\n",
    "        topic_arn = response['TopicArn']\n",
    "        \n",
    "        # Set topic attributes for better identification\n",
    "        sns_client.set_topic_attributes(\n",
    "            TopicArn=topic_arn,\n",
    "            AttributeName='DisplayName',\n",
    "            AttributeValue=description\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Topic '{topic_name}' ready: {topic_arn}\")\n",
    "        return topic_arn\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Error creating topic '{topic_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "# Create success topic\n",
    "success_topic_name = \"async-success\"\n",
    "success_description = \"SageMaker Async Inference Success Notifications\"\n",
    "success_topic_arn = create_sns_topic_if_not_exists(success_topic_name, success_description)\n",
    "\n",
    "# Create error topic  \n",
    "error_topic_name = \"async-failed\"\n",
    "error_description = \"SageMaker Async Inference Error Notifications\"\n",
    "error_topic_arn = create_sns_topic_if_not_exists(error_topic_name, error_description)\n",
    "\n",
    "print(f\"\\nüìß SNS Topics Created Successfully:\")\n",
    "print(f\"Success Topic ARN: {success_topic_arn}\")\n",
    "print(f\"Error Topic ARN: {error_topic_arn}\")\n",
    "\n",
    "print(f\"\\nüîß Topics are ready for AsyncInferenceConfig!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c46610-09ab-4fe3-8881-8bf48cbb1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "endpoint_name=name_from_base('parakeet-asr-async-endpoint')\n",
    "\n",
    "# Create an AsyncInferenceConfig object\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\", \n",
    "    max_concurrent_invocations_per_instance = 4,\n",
    "    failure_path=f\"s3://{bucket}/{prefix}/failed\",\n",
    "    notification_config={\n",
    "        'SuccessTopic' :f\"arn:aws:sns:{sess.boto_region_name}:{sess.account_id()}:async-success\",\n",
    "      \"ErrorTopic\": f\"arn:aws:sns:{sess.boto_region_name}:{sess.account_id()}:async-failed\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Deploy the model for async inference\n",
    "\n",
    "async_predictor = parakeet_model.deploy(\n",
    "    async_inference_config=async_config,\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type ='ml.g5.xlarge', # instance type\n",
    "    endpoint_name = endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7a823-bbfe-4334-a42c-d8e0f872e330",
   "metadata": {},
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "To invoke our endpoint asynchronously, we need to upload our data to S3 first. This is then passed into the `predict_async` function which sends the request to invoke the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1796e-5271-4f00-b09a-ba20f413ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(s3_client, file_path, bucket_name, s3_key):\n",
    "    \"\"\"Upload file to S3\"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {s3_key}: {e}\")\n",
    "        return False\n",
    "s3_client = boto3.client('s3')\n",
    "audio_path = \"../data/test_audio.wav\" \n",
    "s3_key = prefix+f\"/data/{audio_path}\"\n",
    "upload_to_s3(s3_client, audio_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59147f-a9d2-4e87-a40e-10817361ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f\"s3://{bucket}/{s3_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ba631-9b8e-4a85-99cd-6b67040de333",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb772c7-19af-445e-9ad5-37b7aea3bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor_async import AsyncPredictor\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "async_predictor=AsyncPredictor(\n",
    "    Predictor(\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6f5a5-3e96-46dc-80a2-62b9c5f65b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform async inference\n",
    "initial_args = {'ContentType':\"audio/x-audio\"}\n",
    "response = async_predictor.predict_async(initial_args = initial_args, input_path=input_path)\n",
    "response.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6dba4c-e19f-4a9c-b5b2-6c58df8eec3f",
   "metadata": {},
   "source": [
    "Once the data is sent, we wait for the output to be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78658af-1134-4bbf-8c18-44360f7500a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sess.read_s3_file(\n",
    "                        bucket=output_url.netloc, \n",
    "                        key_prefix=output_url.path[1:])\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "                print(\"waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "            \n",
    "output = get_output(response.output_path)\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25cca-f0ea-47bb-9d75-5e0315cba4a8",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Once we are done, we can delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a390043-e8e7-47ad-9984-47997fc8aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52e78e-d28a-4e57-8664-06df90470084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
